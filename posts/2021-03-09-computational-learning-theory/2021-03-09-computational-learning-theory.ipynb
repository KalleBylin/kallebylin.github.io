{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Learning Theory - Notes\n",
    "> A short summary of Computational Learning Theory.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [machine-learning, computational-learning-theory]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "There are good and bad practices for human learning. Highlighting interesting quotes in a book usually makes us feel productive in the moment but, let's be honest, doesn't help much in terms of learning. \n",
    "\n",
    "Taking notes, on the other hand, is a much more effective way of interiorizing information and storing it in long-term memory, especially if we translate concepts with our own words instead of just copying a text.\n",
    "\n",
    "The same is true for learning algorithms, not the part about taking notes but the fact that there ar good and bad methods for learning. Computational Learning Theory (also known as Statistical Learning Theory) is **a branch of computer science focused on understanding how computational learning can happen efficiently and effectively, as well as discovering different impediments to learning**.\n",
    "\n",
    "This document contains my notes on Computational Learning Theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is bias always a bad thing?\n",
    "\n",
    "Bias usually has a very negative connotation, often thought of as prejudice and we are encouraged to always keep an open mind.\n",
    "\n",
    "But don't confuse an open mind with an empty mind. We often learn by building new ideas on top of other ideas we have learned in the past. Keeping an open mind means that we have to remember that some of our foundational building blocks might be wrong and need to be corrected. But constantly throwing away previous knowledge would make learning practically impossible.\n",
    "\n",
    "**Pigeon superstition**\n",
    "\n",
    "One of B.F. Skinner's classical experiments supposedly showed \"superstitious\" behavior by pigeons. The experiment consisted in providing food to hungry pigeons in a cage through a mechanism that was guaranteed to be completely independent from any behavior by the pigeons. \n",
    "\n",
    "> \"If a clock is now arranged to present the food hopper at regular intervals with no reference whatsoever to the bird's behavior, operant conditioning usually takes place. The bird tends to learn whatever response it is making when the hopper appears. The response may be extinguished and reconditioned. The experiment might be said to demonstrate a sort of superstition. The bird behaves as if there were a causal relation between its behavior and the presentation of food, although such a relation is lacking\". \n",
    "\n",
    "*Skinner, B. F. (1948). 'Superstition' in the pigeon. Journal of Experimental Psychology, 38(2), 168â€“172.*\n",
    "\n",
    "Let's say one of the pigeons was pecking on a prticular stain on the bottom of the cage when the first round of food was released. According to Skinner, the pigeon was then more likely to repeat this same behavior which, in turn, also made it more likely for the pigeon to be found pecking on the stain when the next round of food was released.\n",
    "\n",
    "Amusingly, each pigeon showed a different type of behavior that was reinforced (walking around the cage in a specific direction, bobbing the head, etc.) as if the behavior was causing more food to be released. We know that this is not the case, because the food was being released at regular pre-defined intervals.\n",
    "\n",
    "This can then be compared to different forms of human superstition. Have you or anybody you known ever had a special ritual at home before every game of our favorite football team? =D\n",
    "\n",
    "\n",
    "**Rats and selective association**\n",
    "\n",
    "The phenomenon above can be compared to the behavior of rats as studied by Garcia and Koelling in 1966. Imagine rats going for their regular snack. Some of the rats were then induced to feel ill through an injection or radiation. As could be expected these rats then avoided food with similar taste or smell in the future.\n",
    "\n",
    "Another batch of rats was administered a mild electrical shock in the foot. Interestingly, these rats did not show aversion to the food or water, but instead to an audiovisual cue that always happened while they were eating or drinking. \n",
    "\n",
    "This experiment is widely known as one of the first proofs of the so-called Selective Association Effect. This is related to the concept of biological constraints disccused by students of Skinner. Apparently some animals naturally resist certain types of conditioning, as if they had some kind of built in prior knowledge that some types of relationships could not causal. \n",
    "\n",
    "\n",
    "**Inductive bias**\n",
    "\n",
    "This takes us to the concept of inductive bias, which can be described as the set of assumptions or prior knowledge that biases learning by setting restrictions or constraints to the learning process. \n",
    "\n",
    "In our example, the rats seem to be biased towards detecting certain types of patterns between taste or smell and their health, while ignoring other types of seemingly correlated events. \n",
    "\n",
    "This is a well known concept in machine learning. We often talk about models that have high bias or high variance. The second type of models tend to be more complex and are more flexible in the type of functions they can model. But this often makes them more difficult to interpret or prone to overfitting (they memorize the training data, which in this case can be compared to finding superstitious patterns that only apply to that particular set of data).\n",
    "\n",
    "By restricting the types of patterns we are looking for we can often make the learning process easier and also extract valuable insights from the trained model. Linear regression is a great example with well-known assumptions and very powerful applications.\n",
    "\n",
    "**Deductive vs Inductive reasoning**\n",
    "\n",
    "Just as a refresher, we can think about deductive and inductive reasoning as taking to different routes (often described as top-down or bottom-up approaches). \n",
    "\n",
    "On one hand (deduction) we start with general theories about how things should work, we develop a hypothesis that we can then confirm through experiments and specific observations.\n",
    "\n",
    "Deductive reasoning:\n",
    "- Theory -> Hypothesis -> Observations -> Confirmation\n",
    "\n",
    "\n",
    "On the other hand, we might start with specific observations which we then use to try to discover a pattern in those observations. These patterns that we discover can then lead us to a more generalized understanding of the world. \n",
    "\n",
    "Inductive reasoning:\n",
    "- Observations -> Pattern -> Tentative Hypothesis -> Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory\n",
    "\n",
    "In statistics it is common to to have well-defined assumptions about the distribution of our data (e.g. Gaussians). In SLT we have no or very general assumptions.\n",
    "\n",
    "\n",
    "\n",
    "## Is all learning equal?\n",
    "\n",
    "Intuitively it would seem that there are different ways of learning. More formally, we can separate learning by different levels:\n",
    "\n",
    "1. Reproductive learning\n",
    "\n",
    "It could be argued that this first level is not learning at all. \n",
    "\n",
    "Basically, imagine a class where students are allowed to to take all of their notes to class. It would be possible for a student to create a table with all the information he needs without interiorizing the concepts. Once the exam starts he simply looks up the answers to his questions in the table. So he basically reproduces the information.\n",
    "\n",
    "The negative side is that we need to store all of the relevant information beforehand. This can in many cases be very difficult or even impossible. \n",
    "\n",
    "\n",
    "2. Rule-based learning\n",
    "\n",
    "This level of learning can be thought of as encoding the knowledge of experts. Imagine a medical application created to automatically diagnose different types of diseases. One way to do this would be to get input from a group of doctors and create a flow-chart with nodes for every single decision that could be made. \n",
    "\n",
    "Now we don't need to store the raw data or the observations with their corresponding labels as we would do in reproductive learning. Instead we keep all of these rules that can lead us to an answer.\n",
    "\n",
    "The problem with this approach is that it is also very difficult or even impossible to encode every single use case and the rules are often very brittle. \n",
    "\n",
    "\n",
    "3. Creative learning\n",
    "\n",
    "Here past experience is combined in different ways to solve new problems. This can often be a much more powerful approach as we are not limited to only reproducing past observations or following rigid rules.\n",
    "\n",
    "The main issue here is that it can be quite difficult to explain why a decision has been made. \n",
    "\n",
    "\n",
    "> \"If the true classifier is a halfspace, then we should be able to find a very precise separation line with only a few random examples.\"\n",
    "\n",
    "\n",
    "\n",
    "**1-nearest neighbor algorithm**\n",
    "\n",
    "In this extreme example we store all observations and their lables. Any new lable receives the label of the closest point. Two main conclusions we notice are:\n",
    "\n",
    "- This algorithm will always classify all training samples correctly. \n",
    "\n",
    "- This algorithm will also be able to approximate any smooth function, even where halfspace classifiers perform poorly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005 0.025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def true_classifier(point) :\n",
    "    return int(point[0] >= 0)\n",
    "\n",
    "\n",
    "def nearest_neighbor(train_set, test_point):\n",
    "    closest_dist = float('inf')\n",
    "    closest = -1\n",
    "\n",
    "    for features, label in train_set:\n",
    "        dist = sum([(p2-p1)**2 for p1,p2 in list(zip(features, test_point))])\n",
    "        if dist < closest_dist:\n",
    "            closest_dist = dist\n",
    "            closest = label\n",
    "\n",
    "    label = closest\n",
    "    return label\n",
    "\n",
    "\n",
    "def error_probability(train_set) :\n",
    "    mistakes = 0\n",
    "    no_test_points = 2000\n",
    "    for i in range(0,no_test_points):\n",
    "        point = (2*i/no_test_points - 1,)\n",
    "        if true_classifier(point) != nearest_neighbor(train_set, point) :\n",
    "            mistakes += 1\n",
    "    return mistakes/no_test_points\n",
    "\n",
    "train_inputs = [-1.0, -0.1, 0.1]\n",
    "new_input = -0.0001\n",
    "#-0.0010000000000000009\n",
    "\n",
    "S = [((x_val,), true_classifier((x_val,))) for x_val in train_inputs]\n",
    "error_S = error_probability(S)\n",
    "new_point = (new_input,)\n",
    "S.append( (new_point, true_classifier(new_point)) )\n",
    "\n",
    "print(error_S, error_probability(S))\n",
    "assert error_S < error_probability(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
