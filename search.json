[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kalle Bylin",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Trustworthy Online Controlled Experiments\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nA more beautiful question\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nSmart Business\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nPruebas A/B - Fundamentos\n\n\n\n\n\n\n\ndata-science\n\n\nab-testing\n\n\nspanish\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nComputational Learning Theory\n\n\n\n\n\n\n\nmachine-learning\n\n\ncomputational-learning-theory\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning - Notes\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "A short summary of Computational Learning Theory."
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-bias-always-a-bad-thing",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-bias-always-a-bad-thing",
    "title": "Computational Learning Theory",
    "section": "Is bias always a bad thing?",
    "text": "Is bias always a bad thing?\nBias usually has a very negative connotation, often thought of as prejudice and we are encouraged to always keep an open mind.\nBut don’t confuse an open mind with an empty mind. We often learn by building new ideas on top of other ideas we have learned in the past. Keeping an open mind means that we have to remember that some of our foundational building blocks might be wrong and need to be corrected. But constantly throwing away previous knowledge would make learning practically impossible.\nPigeon superstition\nOne of B.F. Skinner’s classical experiments supposedly showed “superstitious” behavior by pigeons. The experiment consisted in providing food to hungry pigeons in a cage through a mechanism that was guaranteed to be completely independent from any behavior by the pigeons.\n\n“If a clock is now arranged to present the food hopper at regular intervals with no reference whatsoever to the bird’s behavior, operant conditioning usually takes place. The bird tends to learn whatever response it is making when the hopper appears. The response may be extinguished and reconditioned. The experiment might be said to demonstrate a sort of superstition. The bird behaves as if there were a causal relation between its behavior and the presentation of food, although such a relation is lacking”.\n\nSkinner, B. F. (1948). ‘Superstition’ in the pigeon. Journal of Experimental Psychology, 38(2), 168–172.\nLet’s say one of the pigeons was pecking on a prticular stain on the bottom of the cage when the first round of food was released. According to Skinner, the pigeon was then more likely to repeat this same behavior which, in turn, also made it more likely for the pigeon to be found pecking on the stain when the next round of food was released.\nAmusingly, each pigeon showed a different type of behavior that was reinforced (walking around the cage in a specific direction, bobbing the head, etc.) as if the behavior was causing more food to be released. We know that this is not the case, because the food was being released at regular pre-defined intervals.\nThis can then be compared to different forms of human superstition. Have you or anybody you known ever had a special ritual at home before every game of our favorite football team? =D\nRats and selective association\nThe phenomenon above can be compared to the behavior of rats as studied by Garcia and Koelling in 1966. Imagine rats going for their regular snack. Some of the rats were then induced to feel ill through an injection or radiation. As could be expected these rats then avoided food with similar taste or smell in the future.\nAnother batch of rats was administered a mild electrical shock in the foot. Interestingly, these rats did not show aversion to the food or water, but instead to an audiovisual cue that always happened while they were eating or drinking.\nThis experiment is widely known as one of the first proofs of the so-called Selective Association Effect. This is related to the concept of biological constraints disccused by students of Skinner. Apparently some animals naturally resist certain types of conditioning, as if they had some kind of built in prior knowledge that some types of relationships could not causal.\nInductive bias\nThis takes us to the concept of inductive bias, which can be described as the set of assumptions or prior knowledge that biases learning by setting restrictions or constraints to the learning process.\nIn our example, the rats seem to be biased towards detecting certain types of patterns between taste or smell and their health, while ignoring other types of seemingly correlated events.\nThis is a well known concept in machine learning. We often talk about models that have high bias or high variance. The second type of models tend to be more complex and are more flexible in the type of functions they can model. But this often makes them more difficult to interpret or prone to overfitting (they memorize the training data, which in this case can be compared to finding superstitious patterns that only apply to that particular set of data).\nBy restricting the types of patterns we are looking for we can often make the learning process easier and also extract valuable insights from the trained model. Linear regression is a great example with well-known assumptions and very powerful applications.\nDeductive vs Inductive reasoning\nJust as a refresher, we can think about deductive and inductive reasoning as taking to different routes (often described as top-down or bottom-up approaches).\nOn one hand (deduction) we start with general theories about how things should work, we develop a hypothesis that we can then confirm through experiments and specific observations.\nDeductive reasoning: - Theory -&gt; Hypothesis -&gt; Observations -&gt; Confirmation\nOn the other hand, we might start with specific observations which we then use to try to discover a pattern in those observations. These patterns that we discover can then lead us to a more generalized understanding of the world.\nInductive reasoning: - Observations -&gt; Pattern -&gt; Tentative Hypothesis -&gt; Theory"
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#statistical-learning-theory",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#statistical-learning-theory",
    "title": "Computational Learning Theory",
    "section": "Statistical Learning Theory",
    "text": "Statistical Learning Theory\nIn statistics it is common to to have well-defined assumptions about the distribution of our data (e.g. Gaussians). In SLT we have no or very general assumptions."
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-all-learning-equal",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-all-learning-equal",
    "title": "Computational Learning Theory",
    "section": "Is all learning equal?",
    "text": "Is all learning equal?\nIntuitively it would seem that there are different ways of learning. More formally, we can separate learning by different levels:\n\nReproductive learning\n\nIt could be argued that this first level is not learning at all.\nBasically, imagine a class where students are allowed to to take all of their notes to class. It would be possible for a student to create a table with all the information he needs without interiorizing the concepts. Once the exam starts he simply looks up the answers to his questions in the table. So he basically reproduces the information.\nThe negative side is that we need to store all of the relevant information beforehand. This can in many cases be very difficult or even impossible.\n\nRule-based learning\n\nThis level of learning can be thought of as encoding the knowledge of experts. Imagine a medical application created to automatically diagnose different types of diseases. One way to do this would be to get input from a group of doctors and create a flow-chart with nodes for every single decision that could be made.\nNow we don’t need to store the raw data or the observations with their corresponding labels as we would do in reproductive learning. Instead we keep all of these rules that can lead us to an answer.\nThe problem with this approach is that it is also very difficult or even impossible to encode every single use case and the rules are often very brittle.\n\nCreative learning\n\nHere past experience is combined in different ways to solve new problems. This can often be a much more powerful approach as we are not limited to only reproducing past observations or following rigid rules.\nThe main issue here is that it can be quite difficult to explain why a decision has been made.\n\n“If the true classifier is a halfspace, then we should be able to find a very precise separation line with only a few random examples.”\n\n1-nearest neighbor algorithm\nIn this extreme example we store all observations and their lables. Any new lable receives the label of the closest point. Two main conclusions we notice are:\n\nThis algorithm will always classify all training samples correctly.\nThis algorithm will also be able to approximate any smooth function, even where halfspace classifiers perform poorly.\n\n\n\nCode\n\ndef true_classifier(point) :\n    return int(point[0] &gt;= 0)\n\n\ndef nearest_neighbor(train_set, test_point):\n    closest_dist = float('inf')\n    closest = -1\n\n    for features, label in train_set:\n        dist = sum([(p2-p1)**2 for p1,p2 in list(zip(features, test_point))])\n        if dist &lt; closest_dist:\n            closest_dist = dist\n            closest = label\n\n    label = closest\n    return label\n\n\ndef error_probability(train_set) :\n    mistakes = 0\n    no_test_points = 2000\n    for i in range(0,no_test_points):\n        point = (2*i/no_test_points - 1,)\n        if true_classifier(point) != nearest_neighbor(train_set, point) :\n            mistakes += 1\n    return mistakes/no_test_points\n\ntrain_inputs = [-1.0, -0.1, 0.1]\nnew_input = -0.0001\n#-0.0010000000000000009\n\nS = [((x_val,), true_classifier((x_val,))) for x_val in train_inputs]\nerror_S = error_probability(S)\nnew_point = (new_input,)\nS.append( (new_point, true_classifier(new_point)) )\n\nprint(error_S, error_probability(S))\nassert error_S &lt; error_probability(S)\n\n\n0.0005 0.025\n\n\n\n\nCode\n\n\n\n[-1, 0]"
  },
  {
    "objectID": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html",
    "href": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html",
    "title": "A more beautiful question",
    "section": "",
    "text": "The Power of Inquiry to Spark Breakthrough Ideas"
  },
  {
    "objectID": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html#introduction-why-questions",
    "href": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html#introduction-why-questions",
    "title": "A more beautiful question",
    "section": "Introduction: Why Questions?",
    "text": "Introduction: Why Questions?\nThe author’s interest in questioning started when he was interviewing some of the most innovative minds for a series of articles and a book he wrote. One common denominator he found was that they were all very good as asking questions.\nFor many of them, their breakthrough products, solutions or services started with a key question or a series of questions that they asked and then answered.\nInterestingly though, most companies and schools don’t teach or encourage good questioning. It is often seen as a waste of time, rebellion towards authority or as a sign of ignorance. Therefore, obedience and memorization is often favored in these settings.\nThe title of the book was borrowed from this quote:\n\n“Always the beautiful answer who asks a more beautiful question” - E. E. Cummings\n\nIt is then worth asking ourselves why questioning is not as common as we could except. To answer this we have to remember that questions tend to disrupt processes and structures that already exist. Questions also force us to wonder if things could be done differently. So, in a way, encouraging people to ask questions implies giving up power.\nAfter a great number of interviews and borrowing from well-known theories in areas like design thinking, the author crated a three-part model to formulate and tackle big, beautiful questions:\n\nWhy?\nWhat if?\nHow?\n\nThe author also finishes the introduction with his own subjective definition of a beautiful question:\n\n“A beautiful question is an ambitious yet actionable question that can begin to shift the way we perceive or think about something and that might serve as a catalyst to bring about change”."
  },
  {
    "objectID": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html#chapter-1-the-power-of-inquiry",
    "href": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html#chapter-1-the-power-of-inquiry",
    "title": "A more beautiful question",
    "section": "Chapter 1: The Power of Inquiry",
    "text": "Chapter 1: The Power of Inquiry\nThis story starts with the emotional but very powerful story about Van Phillips, who lost his leg in a water-skiing accident in the late 1970s. In the hospital he was given a prosthetic leg of wood and foam rubber.\nInspired by the technological advances by NASA and the space program, Phillips asked “Why can’t a prosthetic leg perform more like a human one?” and “Why can’t it bend and flex, enabling a person to run and jump?”\nIn the beginning, many felt these questions as a challenge to the doctors and prosthetics engineers who were experts in the field.\n\n“It [questioning] often has an inverse relationship to expertise. Such that, within their own subject areas, are apt to be poor questioners”.\n\nIn the end, Phillips discovered that he would have to answer the question himself.\nMark Noonan, inventor of the wheeled shovel, has said that if we never do anything about a problem ourselves, then we are not really questioning. We are complaining.\nFast-forwarding several years, Phillips work has impacted the lives of thousands of people and many of us heave seen or heard the story about South Africa’s Oscar Pistorius (aka “the blade runner”), the first double-amputee runner to compete in the Olympics. He ran with a pair of carbon-fiber prosthetic legs known as Cheetahs, created by Van Phillips.\nAfter this, the author focuses on different types of questions and how they have been used in the past.\nFor example, open questions tend to encourage more creative answers than closed questions, even though these are also important.\nThe tone is also very important. When presented with a problem, asking “Oh my God, what are we going to do?” is very different to asking “What if this change represents an opportunity for us?” or “How can we make the most of this situation?”. The second type of questions usually produces better answers.\nDavid Cooperrider, from Case Western Reserve University, is one of the creators of the appreciative inquiry model. This model assumes that the questions we ask tend to focus our attention in a specific direction. One interesting result of this would be that organizations evolve in the direction of the questions they most persistently and passionately ask.\n\n“Forming questions helps us to organize our thinking around what we don’t know” - Stephen Quatrano, The Right Question Institute\n\nThe author also sees a very strong connection between innovation and questioning. In some sense, innovation means trying to find and formulate new questions that can be answered over time. It is very common for new businesses or products to be born from a good question.\nOver time, the value of good questions have been rising. In today’s world it tends to be very easy to find answers in most cases. With so much information available to us, it becomes very difficult to know which questions to ask.\nWe see this same phenomenon with computers, they are extremely efficient at giving us answers we are looking for but they are still not capable of producing valuable questions consistently.\nThe last part of the chapter focuses on the main stages of innovative questioning and the author’s three-part model:\n\nWhy: Confronting, formulating and framing the initial questions that define the problem we have identified. This corresponds to the “why?” moment because we are trying to understand why the problem exists, why it creates an opportunity/need/pain and for whom. We also want to understand why others have not solved it and why it should be important to us. It is common for these “why” questions to be discovered in our daily lives.\nWhat if: We take the understanding we have gained in the previous stage and formulate hypotheses to solve the problem. This is the first step in moving from just asking to action. The author also mentions the concepts of contextual inquiry and connective inquiry. We use the first type when we are trying to get more context about the problem we are interested in. Connective inquiry is the kind of questions we use when we start to combine knowledge from other fields or domains with the problem at hand. This produces questions like “What if a prosthetic leg could have the same strength and flexibility as a springboard so that the person could jump?”\nHow: Decide on a specific solution, build prototypes and/or construct a plan. This is where most of the action happens. Questions tend to be much more practical (e.g. “How do I test this idea?” or “How can I get this to work?”)."
  },
  {
    "objectID": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html#chapter-2-why-we-stop-questioning",
    "href": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html#chapter-2-why-we-stop-questioning",
    "title": "A more beautiful question",
    "section": "Chapter 2: Why we stop questioning",
    "text": "Chapter 2: Why we stop questioning\nThis chapter starts with the apparently never-ending ability of small children to ask questions. Unfortunately, studies show that as they grow up the number of questions that they ask falls drastically and by middle school questioning has practically stopped.\nOne interesting finding is that one main reason children ask why over and over again is because they feel their question has not been answered. In other words, it’s their way of saying “you are not hearing me, you still don’t understand what I’m asking”.\nAs children move into more standardized learning, it seems like we are robbing them of the opportunity to explore and discover questions on their own. Many schools and teachers are tasked with filling up our heads with as many answers as possible based on a list of topics that have to be taught. This leaves very little time for questions that deviate from the topics that have to be covered. Children are forced to sit still in class and memorize as much as possible.\nThis raises the question “what if our schools could train students to be better life-long learners and better adapters to change by enabling them to be better questioners?”\nThis chapter includes the story of Deborah Meier who started applyting experimental approaches to learning in the 60s and 70s in Harlem.\nHer school focused on 5 key learning skills or habits of mind, each of which had been paired to a series of questions:\n\nEvidence: How do we know what is true or false? What evidence counts?\nViewpoint: How might this look like if we were in someone else’s shows or looked at it from another perspective?\nConnection: Is there a pattern? Have we seen something like this before?\nConjecture: What if it were different?\nRelevance: Why does it matter?\n\nHer approach was based on questions, but many viewed this as undisciplined and without structure. Meier’s response to this was that children are easier to control when they have the freedom to focus on what they were interested in."
  },
  {
    "objectID": "posts/2021-04-17-ab-testing-basics-spanish/2021-04-17-ab-testing-basics-spanish.html",
    "href": "posts/2021-04-17-ab-testing-basics-spanish/2021-04-17-ab-testing-basics-spanish.html",
    "title": "Pruebas A/B - Fundamentos",
    "section": "",
    "text": "Un corto resumen sobre pruebas A/B\n\n\n¿Qué es una prueba A/B?\n\nEn pocas palabras, una prueba A/B es una comparación entre dos alternativas con el objetivo de descubrir cuál es mejor.\nCuando tomamos decisiones es muy fácil dejarnos llevar por nuestras opiniones. Esto no siempre es malo, pero es especialmente peligroso en empresas cuando una iniciativa es seleccionada basado principalmente en la opinión de la persona con mayor poder jerárquico en lugar del mérito propio de la iniciativa.\nA lo largo de este documento vamos a presentar también ciertas reglas importantes durante el proceso de comparación que nos permiten tener confianza en el resultado.\nVoy a enfocarme principalmente en decisiones relacionadas al desarrollo de productos digitales, pero es aplicable en muchos otros contextos.\n\nEs una herrmiante de toma de decisiones\nEl valor principal de una prueba A/B nace de la incertidumbre inherente de nuestras decisiones. Cuando no estamos seguros de qué camino tomar, una prueba A/B es una manera efectiva de probar ambas alternativas por un tiempo y dejar que los resultados nos guíen.\n\n\nEs un acelerador de procesos\nEl riesgo y la incertidumbre suelen volver lentos nuestros procesos. Cuando hay riesgo de algo potencialmente dañino para la compañía se vuelve importante crear procesos de verificación y puntos de control antes de lanzar una iniciativa. Desarrollando software es común usar una mezcla entre pruebas automatizadas y revisiones de código.\nEl problema es que por cada persona que tiene que entrar a revisar el trabajo tardas más en sacar la iniciativa a producción y esas personas pierden la posibilidad de estar trabajando en otra cosa.\nUna prueba A/B reduce el riesgo porque te permite controlar el daño potencial y te permite justificar tus decisiones. Esto se puede hacer, por ejemplo, corriendo el experimento con únicamente 5% de tus usuarios.\nAdemás, al comparar los resultados de tu experimento es más fácil darse cuenta si la iniciativa está teniendo un impacto negativo en alguna métrica clave. Sin un punto de comparación suele ser muy difícil saber si un cambio negativo en una métrica se debe a la implementación o algún otro factor.\n\n\nEmpodera personas\nEl uso constante de pruebas A/B reduce la necesidad de que una sola persona tome decisiones y permite a cualquier miembro del equipo aportar ideas. Se vuelve menos importante si alguien piensa que es una buena o mala idea, el desempeño de la idea en el campo de batalla es el juez final srgún los datos recolectados.\n\n\n\nLos experimentos controlados salvan vidas\nLas pruebas A/B son mucho más que una simple herramienta para optimizar la tasa de conversión en una página web.\nEn 1753, el médico naval escocés James Lind publicó su A Treatise of the Scurvy en el que describe una serie de experimentos controlados ofreciendo diferentes soluciones a los miembros de la tripulación. Esto le permitió llegar a la conclusión de que agregar cítricos a la dieta podría ayudar a prevenir y curar el escorbuto.\n\n\n¿Cómo funcionan las pruebas A/B?\n[Trabajo en proceso]"
  },
  {
    "objectID": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html",
    "href": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html",
    "title": "Smart Business",
    "section": "",
    "text": "What Alibaba’s Success Reveals about the Future of Strategy\n\n\n\nAuthor:\nGenre:\nDate finished:\n\n\nImpressions\n\n\nHow the book changed me\n\n\nMy top 3 quotes\n\n\nSummary + Notes"
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html",
    "title": "Deep Learning - Notes",
    "section": "",
    "text": "A short summary of DL fundamentals."
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-perceptron",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-perceptron",
    "title": "Deep Learning - Notes",
    "section": "What is a perceptron?",
    "text": "What is a perceptron?\n\nPerceptrons were originally brain models created to understand how the brain works. A perceptron as we know it encodes several principles about how the brain works and then evolved into an algorithm for supervised binary classification.\n\nIn the 1960’s, Frank Rosenblatt published the book Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. It is curious that this fundamental block for AI was, in the author’s mind, a tool for understanding the human brain and not for pattern recognition (even though he encouraged this use as well):\n\nFor this writer, the perceptron program is not primarily concerned with the invention of devices for “artificial intelligence”, but rather with investigating the physical structures and neurodynamic principles which underlie “natural intelligence”. A perceptron is first and foremost a brain model, not an invention for pattern recognition [emphasis added]” (p. viii).\n\nIn other words, the perceptron is actually a simplification and abstraction which has allowed us to discover principles for how the brain works. These same principles were then also used to create pattern recognition machines.\nRosenblatt explicitly recognizes that his model is a direct descendant of the model created by McCulloch and Pitts, and influenced by the theories of Hebb and Hayek.\nMain components of a perceptron in Rosenblatt’s book: - Environment: The environment generates the information that is initially passed on to the perceptron.\n\nSignal generating units: Each unit receives a signal and generates an output signal.\nSignal propagation functions: These are rules that define how signals are generated and transmitted.\nMemory functions: These are rules that define how properties of the perceptron can be changed in response to certain activity.\n\nThe definition of a single neuron evolves from the ideas above.\nA neuron takes values from its environment (e.g. x1, x2, x3) and each of these gets multiplied by a stored parameter (e.g. w1, w2, w3). The sum of each of these operations is then passed through an activation function.\nIn other words, it’s as if we are trying to pass a signal through the neuron and all of these components work together to establish how the signal is transmitted.\nWe can imagine three old batteries used to turn on a machine. Turning it on with too little energy could cause it to break, so we check the current before passing it on to the machine.\n\n\n\nPerceptron\n\n\nIn the example below, we are randomly initializing the parameters. The activation function is a step-function with a threshold of 4. This means that the signal is only passed on as a unitary value if it is larger than or equal to the threshold.\n\n\nCode\nimport numpy as np\n\nw1 = np.random.random()*2 # generates random floats between 0 and 2\nw2 = np.random.random()*2\nw3 = np.random.random()*2 \n\nprint(f'W1 is equal to: {w1}')\nprint(f'W2 is equal to: {w2}')\nprint(f'W3 is equal to: {w3}\\n')\n\nx1 = 2\nx2 = 1\nx3 = 3\nb = 0\n\nactivation_function = lambda x: 1 if x &gt;=4 else 0\n\noutput = activation_function(x1*w1 + x2*w2 + x3*w3)\n\nprint('Output:', output)\n\nif output == 1:\n    print('The machine is on!')\nelse:\n    print('Not enough energy to turn on the machine :(')\n\n\nW1 is equal to: 0.6321002815675523\nW2 is equal to: 1.6995522179113192\nW3 is equal to: 1.2072466687862997\n\nOutput: 1\nThe machine is on!"
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-an-activation-function",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-an-activation-function",
    "title": "Deep Learning - Notes",
    "section": "What is an activation function?",
    "text": "What is an activation function?\nGoing back to Rosenblatt’s book, activation functions are essentially signal propagation functions.\nWhen a neuron receives a signal, the activation function decides if the signal is passed on and how strong the output signal becomes.\nWe already learned about the step function. This activation is often not ideal for multiple reasons.\nFirst of all, the result is binary. But sometimes we are more interested in also knowing the degree of certainty, so I probability might be better.\nWe might also want to have a wider range of values. When predicting age, for example, binary values of 0 or 1 will be of little value.\nA lot of different activation functions have been developed by researchers. Three common activation functions are:\nSigmoid function Has a great property of having outputs between 0 and 1 and therefore can be interpreted as probabilities. The function is also smooth and easy to differentiate which makes learning easier.\nIt can be problematic when the input signal has very large positive or negative values. At those points the derivative is very close to 0 and learning becomes very slow.\nHyperbolic function This is another smooth function with a range of values between -1 and 1. This is interesting because sometimes a signal might have a reverse effect on the output and the hyperbolic function allows us to include this type of relationship in the network.\nRectified linear unit (ReLU) This function is very simple aned fast to compute. This allows us to work with larger and more complex models that, given enough data, can produce better results overall."
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-neural-network",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-neural-network",
    "title": "Deep Learning - Notes",
    "section": "What is a neural network?",
    "text": "What is a neural network?\n\nA neural network is a directed system of connected neurons which is capable of propagating a signal received from the environment and produces an output signal.\n\nThe neurons are generally organized in different layers which allow us to break free from linearity.\nLinearity means that we are also assuming monotonicity. In other words, an increase in an input value always increases the output value if the corresponding weight is positive (and will always decrease the output value if the weight is negative).\nThis works fine in examples like the one seen above where we turn on a lightbulb depending on how much energy we get from each battery.\nBut this makes less sense in other cases. For examples very pronounced sales spikes in an online marketplace followed by no sales at all could be indicative of fraud. A positive value for a feature describing this type of spikes should increase the probability of fraud.\nBut is this always the case?\nWhat if we additionally know that the owner of the store is a web celeb (网红) on Alibaba? In that case the spikes above are expected, and therefore a positive value for this feature should decrease the probability of fraud while any other behavior might be suspicious.\nLet’s build a neural network step by step:\nThe simplest example is a single neuron with only one parameter which does not modify the signal in any way:\n\n\nCode\ndef nn(x, w):\n    return w*x\n\nprint(nn(3, 6))\n\n\n18\n\n\nMost processes we find in the real world are not this simple. We need to take into account multiple input features.\nI love music, imagine I want to predict if I will like a particular song or not. We can quickly multiply each input value by its corresponding weight using a dot product.\n\n\nCode\ndef nn(X, W):\n    return W.dot(X)\n\nx1 = 3.5 # length of songs in minutes\nx2 = 0 # binary value indicating if the genre is jazz or not\nx3 = 0 # binary value indicating if the artist is Nicki Minaj or not\nx4 = 1 # binary value indicating if the artist is Alan Walker\n\nX = np.array([x1, x2, x3, x4])\nW = np.array([1., 0.3, -3, 4])\n\nprint(nn(X, W))\n\n\n7.5\n\n\nWhat is interesting above is that the weights actually encode certain information about my taste in music. I really don’t like when a song is too short and, in general, I’m not fond of music by Nicki Minaj (notice the negative weight). But I do love music by Alan Walker. Jazz is nice but I don’t love it.\nNow the real question is, how do we know the values of these weights. In this case, I used my deep expertise regarding my own music taste to set the weights. But we would usually be more interested in doing this for the users of our streaming platform at scale. Millions of people we have never and will probably never meet in person.\nSo, first we’re going to change the output. We want to predict if a particular user is going to like a song or not. Using the sigmoid function we can directly compare the true values (0 or 1) to our predictions (floats between 0 and 1).\nWe will also initialize the weights randomly and print out an error value showing how close our prediction is to the true value:\n\n\nCode\n\ndef nn(X, y, W): \n    prediction = W.dot(X)\n    error = y-prediction\n    \n    print('Predicted value:', round(prediction, 3))\n    print('Error:', round(error, 3))\n\nx1 = 3.5 # length of songs in minutes\nx2 = 0 # binary value indicating if the genre is jazz or not\nx3 = 0 # binary value indicating if the artist is Nicki Minaj or not\nx4 = 1 # binary value indicating if the artist is Alan Walker\n\nX = np.array([x1, x2, x3, x4])\ny = 5\nW = np.random.random(size=4)*2-1\n\nnn(X, y, W)\n\n\nPredicted value: -1.468\nError: 6.468\n\n\nIt’s now time to start talking about learning or how our neural network is going to choose the best parameters during training to make accurate predictions.\nWe will follow a very simple framework of three steps:\n\nPredict\nCompare\nLearn\n\nThis time we will add multiple iterations to our algorithm and we will do the three steps above during each iteration.\n\n\nCode\ndef nn(X, y, W): \n    print(f'True value: {y}\\n')\n    \n    for i in range(10):\n        prediction = W.dot(X)\n        error = y-prediction\n\n        print('Predicted value:', round(prediction, 3))\n        print('Error:', round(error, 3))\n        \n        if error &gt; 0:\n            W += 0.01\n        else:\n            W -= 0.01  \n\nx1 = 3.5 # length of songs in minutes\nx2 = 0 # binary value indicating if the genre is jazz or not\nx3 = 0 # binary value indicating if the artist is Nicki Minaj or not\nx4 = 1 # binary value indicating if the artist is Alan Walker\n\nX = np.array([x1, x2, x3, x4])\ny = 1\nW = np.random.random(size=4)*2-1\n\nnn(X, y, W)\n\n\nTrue value: 1\n\nPredicted value: 0.116\nError: 0.884\nPredicted value: 0.161\nError: 0.839\nPredicted value: 0.206\nError: 0.794\nPredicted value: 0.251\nError: 0.749\nPredicted value: 0.296\nError: 0.704\nPredicted value: 0.341\nError: 0.659\nPredicted value: 0.386\nError: 0.614\nPredicted value: 0.431\nError: 0.569\nPredicted value: 0.476\nError: 0.524\nPredicted value: 0.521\nError: 0.479\n\n\nHere we are doing hot & cold learning. After each prediction we are making a comparison just like in the classical game so that we get “hotter” or closer to the true answer after each guess.\nThe way we are calculating the error makes it negative if it was too high or positive if our guess was too low.\nWe can now be a bit smarter in the way we learn. Instead of guessing and trying to jiggle the output to both sides, we can use the gradient to guide us:\n\n\nCode\ndef nn(X, y, W): \n    print(f'True values: {y}\\n')\n    n = y.shape[1]\n    \n    for i in range(100):\n        predictions = W.dot(X.T)\n        mse = (1/2)*np.mean((y-predictions)**2)\n        \n        if i % 10 == 0:\n            print('Mean Squared Error:', round(mse, 3))\n        \n        dW = -(1/n)*(y-predictions).dot(X)\n        W -= 0.1*dW\n          \n    print('\\nW:', np.round(W, 3))\n    print('\\nFinal predictions:', np.round(predictions, 2))\n\nX = np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [1, 1]])\n\ny = np.array([[1, 0, 0, 1]])\nW = np.random.randn(1, 2)\n\nnn(X, y, W)\n\n\nTrue values: [[1 0 0 1]]\n\nMean Squared Error: 0.234\nMean Squared Error: 0.132\nMean Squared Error: 0.078\nMean Squared Error: 0.046\nMean Squared Error: 0.028\nMean Squared Error: 0.017\nMean Squared Error: 0.01\nMean Squared Error: 0.006\nMean Squared Error: 0.004\nMean Squared Error: 0.002\n\nW: [[0.927 0.073]]\n\nFinal predictions: [[0.92 0.07 0.   1.  ]]\n\n\nThis version led us quickly very close to the correct answers. But we now have another problem. The previous problem was very easy to learn because one of the features had a direct 1-on-1 relationship with an output. A simple linear function was capable of leveraging that feature.\nLet’s look at the next example:\n\n\nCode\nX = np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [1, 1]])\n\ny = np.array([[1, 1, 0, 0]])\nW = np.random.randn(1, 2)\n\nnn(X, y, W)\n\n\nTrue values: [[1 1 0 0]]\n\nMean Squared Error: 0.863\nMean Squared Error: 0.572\nMean Squared Error: 0.408\nMean Squared Error: 0.311\nMean Squared Error: 0.254\nMean Squared Error: 0.219\nMean Squared Error: 0.198\nMean Squared Error: 0.186\nMean Squared Error: 0.178\nMean Squared Error: 0.174\n\nW: [[0.462 0.204]]\n\nFinal predictions: [[0.47 0.2  0.   0.67]]\n\n\nMSE might not be extremely high but the answers are terrible. This is because the new problem we’re working with cannot be solved with a simple linear function. So, to solve this we can add a hidden layer with an activation function (more on why this works can be found further down):\n\n\nCode\nnp.random.seed(2)\n\ndef relu(x):\n    return (x&gt;0)*x\n\ndef relu_prime(x):\n    return x&gt;0\n\ndef nn(X, y, hidden_size , lr=0.01): \n    print(f'True values: {y}\\n')\n    n = y.shape[1]\n    \n    W1 = np.random.randn(X.shape[1], hidden_size)\n    W2 = np.random.randn(hidden_size, 1)\n    \n    for i in range(100):\n        z1 = X.dot(W1)\n        a1 = relu(z1)\n        \n        z2 = a1.dot(W2) # predictions\n        mse = (1/2)*np.mean((y-z2)**2)\n        \n        if i % 10 == 0:\n            print('Mean Squared Error:', round(mse, 3))\n        \n        dZ2 = (y-z2)\n        dW2 = a1.T.dot(dZ2)\n        W2 += lr*dW2\n        \n        dZ1 = (dZ2.dot(W2.T))*relu_prime(a1)\n        dW1 = X.T.dot(dZ1)\n        W1 += lr*dW1 #(1/n)\n    \n    print('\\nW1:', np.round(W1, 3))\n    print('W2:', np.round(W2, 3))\n    \n    print('\\nFinal predictions:', np.round(z2, 2))\n\nX = np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [1, 1]])\n\ny = np.array([[1, 1, 0, 0]]).T\n\nnn(X, y, 4)\n\n\nTrue values: [[1]\n [1]\n [0]\n [0]]\n\nMean Squared Error: 1.12\nMean Squared Error: 0.284\nMean Squared Error: 0.133\nMean Squared Error: 0.078\nMean Squared Error: 0.052\nMean Squared Error: 0.037\nMean Squared Error: 0.027\nMean Squared Error: 0.021\nMean Squared Error: 0.016\nMean Squared Error: 0.012\n\nW1: [[-0.417 -0.056 -2.136  0.599]\n [-1.793 -0.842  0.847 -1.302]]\nW2: [[-1.058]\n [-0.909]\n [ 0.876]\n [ 1.738]]\n\nFinal predictions: [[1.04]\n [0.74]\n [0.  ]\n [0.  ]]"
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-universality",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-universality",
    "title": "Deep Learning - Notes",
    "section": "What is universality?",
    "text": "What is universality?\n\nUniversality is the ability to compute any arbitrary function.\n\nIt has been proven that any function we can think of can be approximated by a neural network with at least two layers.\nMichael Nielsen clarifies two caveats related to this idea of universality: 1. Universality doesn’t mean that the neural network is guaranteed to be exact, instead we are approximating the function and the more neurons we add the closer it gets to the true function. This means that for any function f(X) and any error threshold we define we can always find a neural network with output g(x) that satisfies:\n\\[ |g(x) - f(x)| &lt; \\epsilon \\]\n\nNeural networks approximate continuous functions. Functions that are not continuous with sharp and sudden jumps won’t be approximated by a neural network as a general rule. But that doesn’t mean that we can often use a continuous approximation that is good enough for our given purposes when trying to approximate a discontinuous function."
  },
  {
    "objectID": "posts/2021-05-01-trustworthy-controlled-experiments/2021-05-01-trustworthy-controlled-experiments.html",
    "href": "posts/2021-05-01-trustworthy-controlled-experiments/2021-05-01-trustworthy-controlled-experiments.html",
    "title": "Trustworthy Online Controlled Experiments",
    "section": "",
    "text": "A Practical Guide to A/B Testing"
  },
  {
    "objectID": "posts/2021-05-01-trustworthy-controlled-experiments/2021-05-01-trustworthy-controlled-experiments.html#part-1-introductory-topics-for-everyone",
    "href": "posts/2021-05-01-trustworthy-controlled-experiments/2021-05-01-trustworthy-controlled-experiments.html#part-1-introductory-topics-for-everyone",
    "title": "Trustworthy Online Controlled Experiments",
    "section": "Part 1: Introductory topics for everyone",
    "text": "Part 1: Introductory topics for everyone\n\n1. Introduction and motivation\nThe book starts with an anecdote from Bing where an employee had proposed a new idea for how ad headlines should be displayed. The basic idea was to make the title longer by combining it with text from the first line under the title. Nobody seemed to think much of this idea and it was stashed into the backlog of experiments for six months before a software engineer decided to test the idea.\nThis simple change ended up increasing revenue by 12%, which translated to more than $100 million dollars in the US alone annually without hurting other important metrics.\nA few takeaways from this event are that it’s very hard to predict the value of an idea (this one ended up being forgotten during 6 months) and small changes can have huge impacts. For this reason it is important to have infrastructure in place that makes experimenting very cheap. Bing runs more than 10.000 experiments per year and the great majority don’t have this kind of results.\nThere are many types of controlled experiments. The main focus of this book is on a specific type of controlled experiment called an A/B test. The basic idea is to split users randomly between a control version and a variant.\nAnother very important concept is the Overall Evaluation Criterion (OEC). This could be a single metric or a combination of different metrics. The most important thing to remember is that the OEC has to be measurable in the short-term and believed to have a causal relationship with long-term success according to established strategic objectives.\nIt is also important to thoughtfully set up the randomization process. This is the secret sauce of controlled experiments. The basic intuition behind this is that if there is no other factor influencing assignment apart from proper randomization, then on average we would expect both groups to be similar on all factors except the parameter we are changing. This means that any observed effect on the OEC should with very high probability be an effect of that change and nothing else.\nThis leads us into a conversation around correlation, causality and trustworthiness. The authors present a slightly silly but true example by sharing that the number of error messages has a strong inverse correlation with churn. Should we then increase the number of error messages or intentionally add bugs to the code of Office 365? Obviously not, because error messages don’t cause lower churn. It turns out that power users tend to churn less and bump into more error messages just because they use the product more than other users.\n\n“We believe online controlled experiments are the best scientific way to establish causality with high probability”\n\nThe authors list a series of ingredients that need to be present to be able to run a controlled experiment:\n\nExperimental units, like users, that can be assigned to each variant without interference where users in one group could affect the actions of the users of the other group.\nEnough experimental units. The authors recommend to be at least in the thousands. The more units we have the more granular we can be in our experiments and discover smaller effects that are harder to detect.\nKey metrics that are easy to understand, agreed upon and can be measured easily. If possible, the OEC should be used. A proxy metric can be used if the main metric is too difficult or slow to measure.\nEasy changes. The harder it is to create a variant, the harder it becomes to run a controlled experiment. This is one of the reasons software has become an ideal playground for running controlled experiments.\n\nThe authors also present three tenets or key beliefs that any organization needs to run online controlled experiments:\n\nThe organization wants to make data-driven decisions and has formalized an OEC\n\nObviously, most people will say that data-driven decision-making is important for them. Still, in practice it is very common to plan, execute and declare success based on how much of the plan was accomplished while ignoring the impact on key metrics.\nIn contrast, for data-driven decision-making to exist we need a clearly defined OEC (or set of OECs) that is measurable in the short term (1 to 2 weeks) and predictive of long-term success. This way we can quickly evaluate if what we are doing tactically and strategically is actually having a real impact on our OEC.\n\nThe organization is willing to invest in the infrastructure needed to run and test controlled experiments so that results are trustworthy\n\nIn some domains, like healthcare, it can be considered unethical or illegal to run certain controlled experiments. In other domains, like hardware production, changes can be slow and expensive.\nIn general, software is a great option because it tends to be relatively easy to randomly split users between variants, log the results and iteratively add changes to the software. Still, the organization still has to be willing to invest in setting up the necessary tests and processes so that results are trustworthy at scale.\n\nThe organization recongizes that it is poor at assessing the value of ideas\n\nBased on quotes from experts at Microsoft, Google, Netflix, Slack and others, the authors claim that most teams see experiment success rates of 10-33%, with most of them being on the lower end. In other words, we can expect 70-90% of our work to be thrown away due to poor or even negative results.\n\n“Most who have run controlled experiments in customer-facing websites and applications have experienced this humbling reality: we are poor at assessing the value of ideas”.\n\n\n\n2. Running and analyzing experiments\nThis chapter focuses on the principles of designing, running and analyzing experiments.\nIt starts with a story about a hypothetical e-commerce site where some employees have proposed the idea of adding coupon codes to the checkout process but another employee mentions that research has actually shown that coupon codes could have a negative effect (users get distracted or abandon the flow completely until they can get a code).\nThe team decides to try out a fake door or painted door apporach. The analogy is to create a fake door or paint a door on the wall and see how many try to open it. This is a cheap way of testing an idea without having to build the whole feature first.\nNow the team has to choose their OEC. Revenue seems reasonable but the total sum can be misleading because, even if the process is randomized, one variant might end up with more users than the other which would skew the results. So revenue-per-user is a better choice. The authors go even deeper, clearly defining who should be counted as a user in this experiment.\nThe final hypothesis to test would be “Adding a coupon code option to the checkout process has a negative impact on revenue-per-user for users who start the purchase process”\nAt this point it is important to remember a few key concepts related to hypothesis testing:\n\nWhen analyzing the key metric it is important to capture its mean value and standard error. In other words, we usually don’t know the true value of the metric. So we use data estimate a mean value (other statistics can also be used) and the standard error tells us how variable this estimate is.\nSensitivity is the ability to detect statistically significant results (true positives). This concept goes hand in hand with statistical power, the probability of detecting a meaningful result when there really is one. This ability usually gets better with smaller standard errors. This makes intuitive sense. If we have a box filled with balls and almost all of them have the same color (low variablity) we can be pretty certain of the color of a ball if we pick it randomly. If we have a lot of different colors with equal distribution (high variablity) it becomes extremely difficult to guess the correct color. It is often possible to improve sensitivity by exposing more users to the experiment. We have to be careful though because are often additional costs with more users.\nControl vs treatment samples. In this particular type of experiments we are usually not trying to estimate one value directly. We are often more interested in the difference between two values. In this case, we define a Null hypothesis, our assumption of how the world works, as the mean value of our control group being the same as the mean value of our treatment group. If we then look at the data and find this assumption to be unlikely (e.g. there is a very large difference between the sample means) we reject the null hypothesis and say that the difference is statistically significant.\nTo make the previous decision, we calculate the p-value for the difference. This is the probability of observing the difference between our sample means (or a more extreme difference) assuming that the null hypothesis is true. If this probability (the p-value) is very low, it means that observing this difference is very unlikely and that’s why we go on to reject the null hypothesis.\nFor this to work, we have to decide on a threshold before we run our experiment, usually known as the significance level. A common standard is 0.05 which means that if there really is no difference between the two samples then we will correctly conclude this 95 out of each 100 experiments on average. Seen from another perspective, we will conclude that the difference is statistically significant even if there actually was no real difference (a false positive) about 5% of the time (note that this is different to stating that there is a 5% probability of a false positive if we reject the null hypothesis, which is a common misinterpretation).\nFinally, there is also the question of practical significance. For a more personal example, would it be worth it to take on the challenge and costs of moving to another country with no family and friends for a 1% increase in salary keeping everything else equal (job requirements, benefits, etc.)? Probaby not, but a larger difference might actually be worth it. Here the question is, how large does the difference have to be so that it is practically meaningful?\n\nNow that we have a hypothesis, a significance boundary and a metric we also need to answer these questions to finish the design of the experiment:\n\nWhat is the randomization unit? (Users is the most common)\nWhat population of the randomization unit are we going to target? (All users vs a specific segment)\nHow large does the population of our experiment have to be?\nHow long do we have to run the experiment?\n\nThere are multiple factors we can take into account for the size of the population. If the metric is binary (yes/no) or if we don’t care about very granular differences we can use smaller populations. This is also the case for larger p-value thresholds or significance levels. In other words, if we are willing to make more mistakes then we don’t need very large populations.\nOf course, this leads us to other kinds of questions. If the importance of the experiment is very high then we might need larger populations to increase our certainty of the results. The opposite is true if we suspect that the change could have a negative effect on users. In that case we might have to start with a smaller population and slowly increase the size to reduce risk of a negative impact.\nThe duration of the experiment is also impacted by several factors. First of all, if we need more users we often have to allow the experiment to run for a longer period of time because not all users are exposed to the experiment simultaneously.\nA primacy effect might cause users to behave differently when they first see the change so we’ll also have to wait longer to see if this behavior persists or not. It is also important to keep seasonality or day-of-week effects in mind. We might have to run the experiment longer than necessary from a theoretical stanpoint just to include different dates in the experiment.\nThere are two main components for running the experiment:\n\nInfrastructure to run the experiment. This includes the possibility of randomizing, presenting different variants, etc.\nInstrumentation to log how users are interacting with the variants and to calculate how the OEC is affected by the experiment.\n\nBefore analyzing the results, it is important to also run sanity checks on our experiment.\nThis usually includes guardrail metrics and invariants. In the first case we might want to check the sample sizes so that they match with the expected proportions. Some guardrail metrics are also expected to be invariant for the experiments. For example, latency should not change between variants unless the change was specifically designed to alter the latency.\nWith all this in place, we can now interpret the results and make decisions. The principles we have covered exist specifically so we can trust our experiments repeatedly and thus be able to make the right decisions based on the data.\nFor this reason, it is important to understand the business implications of our experiments so that we can modify statistical and practical significance levels before we start the experiment. This includes the costs of building the feature if the experiment is successful, cost of maintenance, effects on other important metrics, etc.\n\n\n3. Twyman’s law and experimentation trustworthiness\n\nTwyman’s Law: “Any statistic that appears interesting is almost certainly a mistake” - Paul Dickson\n\nFrom experience, most extreme results tend to be due to computational errors, loss or duplication of data and instrumentation errors (e.g. logging). This chapter focuses on several common errors and how we can create tests to avoid falling into these mistakes.\n\nLack of statistical power. Here the mistake lies in assuming that there is no effect if the result of a hypothesis test is that a metric is not statistically significant. We simply don’t have enough data to conclude this. For example, an effect can go unnoticed if we have a very small sample size. For that reason we have to understand what effect size is practically important to us and make sure that our test has enough power to detect that effect.\nMisinterpreting p-values. Some common ways of misunderstanding this value is to state that if p-value=0.01 then there is a 1% chance that the null hypothesis is true or that there is a 1% of our result being a false positive after rejecting the null hypothesis. Compare these statements to the definitions above to prove that these conclusions don’t follow from the given definition.\nPeeking at p-values. This is a form of “p-value hacking” where it is common to peek at the p-values of an ongoing experiment (e.g. checking the p-value daily) until it is low enough to be significant and then reporting this value. Doing this consistently tends to bias the results.\nMultiple hypothesis tests. This is a more general version of the previous concept and consists in comparing multiple versions of a test and picking the one with the lowest p-value to declare a significant result. other examples of this is to look at multiple metrics, segments, or even multiple iterations of the same experiment.\nConfidence intervals. The authors present a few misunderstandings with respect to confidence intervals, one of which is assuming that a 95% confidence interval, for example, has a 95% probability of containing the true effect. The true value is either inside our outside the confidence interval, we already mentioned that the confidence interval and significance levels define how often we are willing to incorrectly reject the null hypothesis on average over multiple experiments.\n\nOther common issues are mentioned related to Internal validity (Violations of SUTVA, Survivorship bias, Intention-to-Treat, Sample Ratio Mismatch) and external validity (e.g. primacy and novelty effects)"
  }
]