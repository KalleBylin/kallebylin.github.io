[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kalle Bylin",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "A More Beautiful Question\n\n\n\n\n\n\n\nBooks\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nSmart Business\n\n\n\n\n\n\n\nBooks\n\n\nLeadership\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nComputational Learning Theory - Notes\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning - Notes\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2021\n\n\nKalle Bylin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html",
    "title": "Computational Learning Theory - Notes",
    "section": "",
    "text": "A short summary of Computational Learning Theory."
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-bias-always-a-bad-thing",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-bias-always-a-bad-thing",
    "title": "Computational Learning Theory - Notes",
    "section": "Is bias always a bad thing?",
    "text": "Is bias always a bad thing?\nBias usually has a very negative connotation, often thought of as prejudice and we are encouraged to always keep an open mind.\nBut don‚Äôt confuse an open mind with an empty mind. We often learn by building new ideas on top of other ideas we have learned in the past. Keeping an open mind means that we have to remember that some of our foundational building blocks might be wrong and need to be corrected. But constantly throwing away previous knowledge would make learning practically impossible.\nPigeon superstition\nOne of B.F. Skinner‚Äôs classical experiments supposedly showed ‚Äúsuperstitious‚Äù behavior by pigeons. The experiment consisted in providing food to hungry pigeons in a cage through a mechanism that was guaranteed to be completely independent from any behavior by the pigeons.\n\n‚ÄúIf a clock is now arranged to present the food hopper at regular intervals with no reference whatsoever to the bird‚Äôs behavior, operant conditioning usually takes place. The bird tends to learn whatever response it is making when the hopper appears. The response may be extinguished and reconditioned. The experiment might be said to demonstrate a sort of superstition. The bird behaves as if there were a causal relation between its behavior and the presentation of food, although such a relation is lacking‚Äù.\n\nSkinner, B. F. (1948). ‚ÄòSuperstition‚Äô in the pigeon. Journal of Experimental Psychology, 38(2), 168‚Äì172.\nLet‚Äôs say one of the pigeons was pecking on a prticular stain on the bottom of the cage when the first round of food was released. According to Skinner, the pigeon was then more likely to repeat this same behavior which, in turn, also made it more likely for the pigeon to be found pecking on the stain when the next round of food was released.\nAmusingly, each pigeon showed a different type of behavior that was reinforced (walking around the cage in a specific direction, bobbing the head, etc.) as if the behavior was causing more food to be released. We know that this is not the case, because the food was being released at regular pre-defined intervals.\nThis can then be compared to different forms of human superstition. Have you or anybody you known ever had a special ritual at home before every game of our favorite football team? =D\nRats and selective association\nThe phenomenon above can be compared to the behavior of rats as studied by Garcia and Koelling in 1966. Imagine rats going for their regular snack. Some of the rats were then induced to feel ill through an injection or radiation. As could be expected these rats then avoided food with similar taste or smell in the future.\nAnother batch of rats was administered a mild electrical shock in the foot. Interestingly, these rats did not show aversion to the food or water, but instead to an audiovisual cue that always happened while they were eating or drinking.\nThis experiment is widely known as one of the first proofs of the so-called Selective Association Effect. This is related to the concept of biological constraints disccused by students of Skinner. Apparently some animals naturally resist certain types of conditioning, as if they had some kind of built in prior knowledge that some types of relationships could not causal.\nInductive bias\nThis takes us to the concept of inductive bias, which can be described as the set of assumptions or prior knowledge that biases learning by setting restrictions or constraints to the learning process.\nIn our example, the rats seem to be biased towards detecting certain types of patterns between taste or smell and their health, while ignoring other types of seemingly correlated events.\nThis is a well known concept in machine learning. We often talk about models that have high bias or high variance. The second type of models tend to be more complex and are more flexible in the type of functions they can model. But this often makes them more difficult to interpret or prone to overfitting (they memorize the training data, which in this case can be compared to finding superstitious patterns that only apply to that particular set of data).\nBy restricting the types of patterns we are looking for we can often make the learning process easier and also extract valuable insights from the trained model. Linear regression is a great example with well-known assumptions and very powerful applications.\nDeductive vs Inductive reasoning\nJust as a refresher, we can think about deductive and inductive reasoning as taking to different routes (often described as top-down or bottom-up approaches).\nOn one hand (deduction) we start with general theories about how things should work, we develop a hypothesis that we can then confirm through experiments and specific observations.\nDeductive reasoning: - Theory -&gt; Hypothesis -&gt; Observations -&gt; Confirmation\nOn the other hand, we might start with specific observations which we then use to try to discover a pattern in those observations. These patterns that we discover can then lead us to a more generalized understanding of the world.\nInductive reasoning: - Observations -&gt; Pattern -&gt; Tentative Hypothesis -&gt; Theory"
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#statistical-learning-theory",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#statistical-learning-theory",
    "title": "Computational Learning Theory - Notes",
    "section": "Statistical Learning Theory",
    "text": "Statistical Learning Theory\nIn statistics it is common to to have well-defined assumptions about the distribution of our data (e.g.¬†Gaussians). In SLT we have no or very general assumptions."
  },
  {
    "objectID": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-all-learning-equal",
    "href": "posts/2021-03-09-computational-learning-theory/2021-03-09-computational-learning-theory.html#is-all-learning-equal",
    "title": "Computational Learning Theory - Notes",
    "section": "Is all learning equal?",
    "text": "Is all learning equal?\nIntuitively it would seem that there are different ways of learning. More formally, we can separate learning by different levels:\n\nReproductive learning\n\nIt could be argued that this first level is not learning at all.\nBasically, imagine a class where students are allowed to to take all of their notes to class. It would be possible for a student to create a table with all the information he needs without interiorizing the concepts. Once the exam starts he simply looks up the answers to his questions in the table. So he basically reproduces the information.\nThe negative side is that we need to store all of the relevant information beforehand. This can in many cases be very difficult or even impossible.\n\nRule-based learning\n\nThis level of learning can be thought of as encoding the knowledge of experts. Imagine a medical application created to automatically diagnose different types of diseases. One way to do this would be to get input from a group of doctors and create a flow-chart with nodes for every single decision that could be made.\nNow we don‚Äôt need to store the raw data or the observations with their corresponding labels as we would do in reproductive learning. Instead we keep all of these rules that can lead us to an answer.\nThe problem with this approach is that it is also very difficult or even impossible to encode every single use case and the rules are often very brittle.\n\nCreative learning\n\nHere past experience is combined in different ways to solve new problems. This can often be a much more powerful approach as we are not limited to only reproducing past observations or following rigid rules.\nThe main issue here is that it can be quite difficult to explain why a decision has been made.\n\n‚ÄúIf the true classifier is a halfspace, then we should be able to find a very precise separation line with only a few random examples.‚Äù\n\n1-nearest neighbor algorithm\nIn this extreme example we store all observations and their lables. Any new lable receives the label of the closest point. Two main conclusions we notice are:\n\nThis algorithm will always classify all training samples correctly.\nThis algorithm will also be able to approximate any smooth function, even where halfspace classifiers perform poorly.\n\n\n\nCode\n\ndef true_classifier(point) :\n    return int(point[0] &gt;= 0)\n\n\ndef nearest_neighbor(train_set, test_point):\n    closest_dist = float('inf')\n    closest = -1\n\n    for features, label in train_set:\n        dist = sum([(p2-p1)**2 for p1,p2 in list(zip(features, test_point))])\n        if dist &lt; closest_dist:\n            closest_dist = dist\n            closest = label\n\n    label = closest\n    return label\n\n\ndef error_probability(train_set) :\n    mistakes = 0\n    no_test_points = 2000\n    for i in range(0,no_test_points):\n        point = (2*i/no_test_points - 1,)\n        if true_classifier(point) != nearest_neighbor(train_set, point) :\n            mistakes += 1\n    return mistakes/no_test_points\n\ntrain_inputs = [-1.0, -0.1, 0.1]\nnew_input = -0.0001\n#-0.0010000000000000009\n\nS = [((x_val,), true_classifier((x_val,))) for x_val in train_inputs]\nerror_S = error_probability(S)\nnew_point = (new_input,)\nS.append( (new_point, true_classifier(new_point)) )\n\nprint(error_S, error_probability(S))\nassert error_S &lt; error_probability(S)\n\n\n0.0005 0.025\n\n\n\n\nCode\n\n\n\n[-1, 0]"
  },
  {
    "objectID": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html",
    "href": "posts/2021-04-27-a-more-beautiful-question/2021-04-27-a-more-beautiful-question.html",
    "title": "A More Beautiful Question",
    "section": "",
    "text": "The Power of Inquiry to Spark Breakthrough Ideas\n\nAuthor: Warren Burger\nPurchase on: Amazon\n\nüìñ The book in 3 sentences\n\n\nüé® Impressions\n\n\n‚òòÔ∏è How the book changed me\n\n\n‚úçÔ∏è My top 3 quotes\n\n‚ÄúTo encourage or even allow questioning is to cede power‚Äù\n‚ÄúForming questions helps us to organize our thinking around what we don‚Äôt know‚Äù - Stephen Quatrano, The Right Question Institute\n\n\n\n\nüìí Summary + Notes"
  },
  {
    "objectID": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html",
    "href": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html",
    "title": "Smart Business",
    "section": "",
    "text": "Author: Ming Zeng\nPurchase on: Amazon"
  },
  {
    "objectID": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#introduction-why-you-need-to-know-about-alibaba",
    "href": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#introduction-why-you-need-to-know-about-alibaba",
    "title": "Smart Business",
    "section": "Introduction: Why you need to know about Alibaba",
    "text": "Introduction: Why you need to know about Alibaba\nThe book starts with a series of impressive examples experienced during Singles Day (November 11th) 2017, now one of the largest shopping events in the world.\nThat year Alibaba set a world record for most payment transactions during the festival, Alipay processed 256.000 payment transactions per second, it took 28 seconds to reach 1 billion RMB in sales, and the first package was delivered only 12 minutes after the midnight start of the event.\nIn comparison, the author explains that as of August 2017, the stated capacity of Visa was 65.000 payments per second globally.\n\n‚ÄúAlibaba is not China‚Äôs version of Amazon‚Äù\n\nAlibaba has historically often been described as ‚Äúthe Amazon of China‚Äù (I have personally used this description when talking to others before I lived in China for a semester).\nThe author suggests that we should think of Alibaba as doing what Amazon, eBay, PayPal, Google, FedEx, the wholesalers and a portion of the manufacturers in the United States do.\nBorrowing a quote from Jack Ma (‚Äúe-commerce is the main course in China but only dessert in the US‚Äù), we are led to understand that China‚Äôs weak and undeveloped infrastructure a few decades ago allowed the country to leapfrog ahead of other countries. Without the weight of legacy infrastructure and high switching costs seen in countries like the US, China was able to develop internet-native retailing, payment and logistics ecosystems.\nThis is not unique to China, other countries with undeveloped infrastructures have started to follow this example. The author claimed that Alipay is becoming the standard for mobile payments across Asia and that Alibaba‚Äôs e-commerce model is expanding rapidly in India.\nOne of the major drivers of the disruptive ecosystems in China, is the use of cutting-edge technology like cloud computing and machine learning.\nThe author‚Äôs definition of smart business:\n\nI call this strategy of embracing new technology to connect all your players and redesign industries smart business.\n\nFor example, coordinating business activity across an extremely large number of interconected players is impossible without automating a lot of actions and decisions. This is made possible using tools like machine learning.\n\n\n\n\n\n\nNote\n\n\n\nFor more on the role of machine learning in predictions and decision-making, see my notes on the book ‚ÄúPrediction machines‚Äù"
  },
  {
    "objectID": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#part-one---alibaba-the-emergence-of-a-smart-business",
    "href": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#part-one---alibaba-the-emergence-of-a-smart-business",
    "title": "Smart Business",
    "section": "Part one - Alibaba: The emergence of a Smart Business",
    "text": "Part one - Alibaba: The emergence of a Smart Business\n\nThe new forces of value creation\nThe mission of Alibaba is ‚ÄúTo make it easy to do business anywhere‚Äù. This has guided the company in creating the technology infrastructure to allow merchants and businesses to engage with their customers.\nThe essence of smart business:\n\nNetwork Coordination + Data Intelligence = Smart Business\n\nComplicated business activity, which was previously locked into rigid and vertically integrated structures, can now be broken down using networked approaches thanks to reduced transaction costs enabled by technology.\nBusinesses can coordinate automatically, which allows for descentralized, scalable and optimized processes.\nAutomatic coordination is usually digital, which makes it possible to collect data and leverage what the author calls data intelligence: ‚Äúbusiness capability of effectively iterating products and services according to consumer activity and response‚Äù.\nChinese companies are better positioned to take advantage of network coordination due to the internet-native infrastructure described above, while companies in the US excel at data intelligence.\nIn this environment, the familiar forces of competition lose importance in favor of new forms of coordination, this means that the ways of creating value are completely transformed.\n\n\nNetwork Coordination\nThe current distruptive nature of China‚Äôs logistics ecosystem was born out of distasters and major headaches, such as Singles Day in 2012.\nThe author attributes great part of the rapid evolution of China‚Äôs logistics infrastructure to network coordination, such that multiple players learned to coordinate efficiently and at scale leveraging internet platforms and data.\nNetwork coordination in Alibaba was born out of scarcity of resources. For example, they could not start their own delivery company to avoid the country‚Äôs slow and outdated postal system. Instead, engineers built standard tools to allow the integration of these services to Alibaba‚Äôs platform and encouraged other players to create the services.\nThe focus of the company shifted toward managing this coordination of a large network of players instead of spending most of the time transmitting information vertically between suppliers and customers.\nBusiness networks are formed when multiple players unite to solve a complex commercial problem for a client base. The author provides four building blocks of coordinated networks based on their experience with Taobao:\n\nDirect connection and interaction. This is one of the key factors which helped Taobao beat eBay in China.\nRole evolution. The network needs to develop, and can‚Äôt be completely planned. The defintions of participants‚Äô roles need to be fuzzy at first. They are then allowed to evolve and when they solidify they can receive official support. Too rigid definitions can limit the network‚Äôs growth.\nInvestment in infrastructure like APIs, search functionality, reputation systems, etc.\nPutting business activities online or business ‚Äúsoftwaring‚Äù.\n\nOne of the major benefits of the network, enabled by the principles above, is flexibility.\nExamples of network coordination that are easier to relate to in the West are Wikipedia, the open-source movement, etc.\n\n\nData intelligence\nLeveraging data to automate actions and decisions opens up new opportunities and scale.\n\nNo one assigns an Uber car to a rider, and no Taobao associate recommends a dress; the algorithms do it. Although there is an enormous amount of human effort and creativity involved in creating these services, once that effort is done, the business practically runs itself.\n\nAs an example, the author describes the use of data and machine learning in Alibaba to kick-start within Alipay an effective, scalable and profitable SME lending business.\nThree cornerstones data intelligence to operate:\n\nAdaptable products\nDatafication (similar to digitalization but is used by the author to emphasize a greater breadth of types of data)\nMachine learning"
  },
  {
    "objectID": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#part-two---how-smart-businesses-compete-strategic-principles",
    "href": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#part-two---how-smart-businesses-compete-strategic-principles",
    "title": "Smart Business",
    "section": "Part two - How smart businesses compete: Strategic principles",
    "text": "Part two - How smart businesses compete: Strategic principles\nThis part describes the core principles of smart business, such as ‚Äúsoftwaring‚Äù workflows to automate decision-making.\nTo make smart business possible it is critical that the business model is re-aligned around customer, the author calls this the consumer-to-business model.\n\nAutomating decisions\nFive steps:\n\nDatafy the physical world. Events and processed need to be encoded so that they can be understood by computers.\nSoftware the business. ‚ÄúSoftwaring‚Äù is the process of retooling a business, its people and its resources using software so that it can achieve network coordination and data intelligence.\nGet data flowing and introduce APIs.\nRecord data in full\nApply machine learning algorithms\n\nOur understanding of a business activity determines how it gets turned into data and this defines which products and services can be created to solve the business problem.\n\n\nThe Customer-to-Business model\nThis C2B model is in direct contrast to the traditional B2C model. This is because when machine learning drives business decisions through feedback loops based on customer behavior, then the customers are effectively dicating the company‚Äôs actions.\nFor this to be possible, companies need to decouple processes so that they are functionally independent but allow them to integrate automatically.\nThis part of the book shows multiple examples of C2B models in China in great detail, such as the ‚Äúweb celebs‚Äù: influencers that have learned to leverage the platform together with other players which allow them to compete with large brands.\nInstead of using ‚Äúcustomer first‚Äù as a slogan (they are often actually ‚Äúcompany first‚Äù), C2B companies are truly ‚Äúcustomer first‚Äù by design.\nThe author offers four general principles for C2B-aligned operations:\n\nDevelop a smart network. C2B companies are usually ‚Äúsmart businesses‚Äù leveraging network coordination and data intelligence.\nDesign the right internet interface. C2B is usually more pull than push, so customers need an interface where they can share their needs and feedback, ideally in real time and at very low cost.\nBuild a C2B beachhead. Start with one module that can get the momentum going.\nUse the capabilities of platforms.\n\n\n\nPositioning\nEcosystems built around smart business are described with a new framework: points, lines and planes.\nTraditional positioning asks: Who are your customers? What are your value propositions? How is your positioning different from that of your competitors?\nIn response to this, Michael Porter proposed three positioning strategies: cost leadership, differentiation and niche.\nWithin a smart network positioning is described differently, internally in Alibaba a geometrical analogy is used:\n\nPoints are individuals or firms with specialized skills providing functional services but that can‚Äôt survive on their own (e.g., factories, designers, models, etc.)\nLines use the services provided by points and planes to create products and services combining productive functions and capabilities. For example, a web celeb incubator or even the web celebs.\nPlanes are platforms like Taobao that provide infrastructural services and help new lines to form and grow.\n\nEach have different value propositions and competitive advantages. For example, while the competitive advantage of points is often expertise, lines have velue/cost/efficiency and planes have to be good at matching.\nWith this framework, the author explains that when Taobao is compared to Amazon, it is being mistakenly classified as a line firm when it is actually a plane.\nTraditionally, points have been absorbed into larger organizations to reduce transaction costs, but planes reduce the costs and create markets for easy exchange of skills. It is also now much easier for them to scale up and become profitable very quickly.\nIt is also important to understand that points, lines and planes are interdependent. Players that try to build their business model on their own will probably be outplayed by players that leverage the network effectively."
  },
  {
    "objectID": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#part-three---how-smart-businesses-run-organizational-implications",
    "href": "posts/2021-04-18-smart-business/2021-04-18-smart-business.html#part-three---how-smart-businesses-run-organizational-implications",
    "title": "Smart Business",
    "section": "Part three - How smart businesses run: Organizational implications",
    "text": "Part three - How smart businesses run: Organizational implications\nWith a new strategic framework, it is natural for management to also evolve.\n\nSelf-tuning\n\nThe classical approach of analyze, plan, and execute is much too slow and inflexible for today‚Äôs environment. Instead of formal planning, strategy formulation is the constant and rapid iteration between vision and action.\n\nThe idea of self-tuning is borrowed from the iterative learning of machine learning algorithms. Strategic departmens shift their focus towards creating a continuous loop of experimentation and learning.\nAlibaba discovered that they did not know enough about the future to plan for the next few years. There was a real need to allow the company to adjust and adapt in real time, without traditional management getting in the way.\nComputer algorithms do not program themselves, it is humans that must decide their objective function which defines how the algorithm prioritizes different directions toward that goal.\nIn Alibaba, mission is defined as a relatively fixed reason to exist while the vision correspondes to a mutable, improvable view of the future.\nFor a company, the equivalent of an objective function is the vision. It sets the direction of the evolution the firm and the network it belongs to.\nAs time passes, the vision has to be checked against reality and updated. The objective function may need to be recalibrated, improved or changed.\nA powerful example of this experimentation mentality comes from 2011, when there was a heated internal debate on which business model to build up.\nManagement decided to split the successful business into three independent and competing units (Tmall, Taobao & Etao) and let the market pick the future winners. By 2013, Tmall was the clear market leader.\nIt was a hard decision, with high organizational and financial costs. Explaining the experiment to employees and what they were trying to learn was crucial.\n\nInstead of micromanaging the firm, management creates the organization‚Äôs architecture to run itself.\n\nThe book covers examples on how smart business applied this new strategic focus in the areas of\n\nPeople. The success of smart business depends on creative workers making mission and culture even more important.\nInfrastructure. The right infrastructure is needed to enable people instead of simply manage them. For example, in many companies and teams the cost of experimentation is prohibitive.\nMechanisms. Coordination mechanisms such as software or platforms that enable teams to collaborate with each other.\n\n\n\nThe future of smart business\nThe last chapter summarizes many of the lessons from Alibaba‚Äôs experience.\nA repeating theme in these experiences and throughout the book is the importance of feedback loops.\nFeedback is necessary for effective learning and fast feedback loops speed up learning.\nAnother important concept is ecosystem, which the author admits has been overused the last few decades. It is still a useful metaphor because strategy for platforms is interconnected instead of isolated and reactive instead of planned.\nThe speed in which the world changes is accelerating. This makes it very difficult to predict the future, and this makes it even more important to try to find a clear view of the future. This process of visioning is something anyone can do and we can then self-tune by testing our vision with actions that give us relevant feedback.\nSimilar to the knowledge revolution described by Peter Drucker, management at Alibaba see a creativity revolution in progress. Innovation and human creativity become key components of producing value while routine work, including information processing will have decresasing value.\nPlatforms succeed when they help individuals grow and succeed. For example, the web-celeb example shows how someone with limited earning power and freedom working for a large company as a normal in-house model can become a freelancer and then a brand owner through the Taobao platform."
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html",
    "title": "Deep Learning - Notes",
    "section": "",
    "text": "A short summary of DL fundamentals."
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-perceptron",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-perceptron",
    "title": "Deep Learning - Notes",
    "section": "What is a perceptron?",
    "text": "What is a perceptron?\n\nPerceptrons were originally brain models created to understand how the brain works. A perceptron as we know it encodes several principles about how the brain works and then evolved into an algorithm for supervised binary classification.\n\nIn the 1960‚Äôs, Frank Rosenblatt published the book Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. It is curious that this fundamental block for AI was, in the author‚Äôs mind, a tool for understanding the human brain and not for pattern recognition (even though he encouraged this use as well):\n\nFor this writer, the perceptron program is not primarily concerned with the invention of devices for ‚Äúartificial intelligence‚Äù, but rather with investigating the physical structures and neurodynamic principles which underlie ‚Äúnatural intelligence‚Äù. A perceptron is first and foremost a brain model, not an invention for pattern recognition [emphasis added]‚Äù (p.¬†viii).\n\nIn other words, the perceptron is actually a simplification and abstraction which has allowed us to discover principles for how the brain works. These same principles were then also used to create pattern recognition machines.\nRosenblatt explicitly recognizes that his model is a direct descendant of the model created by McCulloch and Pitts, and influenced by the theories of Hebb and Hayek.\nMain components of a perceptron in Rosenblatt‚Äôs book: - Environment: The environment generates the information that is initially passed on to the perceptron.\n\nSignal generating units: Each unit receives a signal and generates an output signal.\nSignal propagation functions: These are rules that define how signals are generated and transmitted.\nMemory functions: These are rules that define how properties of the perceptron can be changed in response to certain activity.\n\nThe definition of a single neuron evolves from the ideas above.\nA neuron takes values from its environment (e.g.¬†x1, x2, x3) and each of these gets multiplied by a stored parameter (e.g.¬†w1, w2, w3). The sum of each of these operations is then passed through an activation function.\nIn other words, it‚Äôs as if we are trying to pass a signal through the neuron and all of these components work together to establish how the signal is transmitted.\nWe can imagine three old batteries used to turn on a machine. Turning it on with too little energy could cause it to break, so we check the current before passing it on to the machine.\n\n\n\nPerceptron\n\n\nIn the example below, we are randomly initializing the parameters. The activation function is a step-function with a threshold of 4. This means that the signal is only passed on as a unitary value if it is larger than or equal to the threshold.\n\n\nCode\nimport numpy as np\n\nw1 = np.random.random()*2 # generates random floats between 0 and 2\nw2 = np.random.random()*2\nw3 = np.random.random()*2 \n\nprint(f'W1 is equal to: {w1}')\nprint(f'W2 is equal to: {w2}')\nprint(f'W3 is equal to: {w3}\\n')\n\nx1 = 2\nx2 = 1\nx3 = 3\nb = 0\n\nactivation_function = lambda x: 1 if x &gt;=4 else 0\n\noutput = activation_function(x1*w1 + x2*w2 + x3*w3)\n\nprint('Output:', output)\n\nif output == 1:\n    print('The machine is on!')\nelse:\n    print('Not enough energy to turn on the machine :(')\n\n\nW1 is equal to: 0.6321002815675523\nW2 is equal to: 1.6995522179113192\nW3 is equal to: 1.2072466687862997\n\nOutput: 1\nThe machine is on!"
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-an-activation-function",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-an-activation-function",
    "title": "Deep Learning - Notes",
    "section": "What is an activation function?",
    "text": "What is an activation function?\nGoing back to Rosenblatt‚Äôs book, activation functions are essentially signal propagation functions.\nWhen a neuron receives a signal, the activation function decides if the signal is passed on and how strong the output signal becomes.\nWe already learned about the step function. This activation is often not ideal for multiple reasons.\nFirst of all, the result is binary. But sometimes we are more interested in also knowing the degree of certainty, so I probability might be better.\nWe might also want to have a wider range of values. When predicting age, for example, binary values of 0 or 1 will be of little value.\nA lot of different activation functions have been developed by researchers. Three common activation functions are:\nSigmoid function Has a great property of having outputs between 0 and 1 and therefore can be interpreted as probabilities. The function is also smooth and easy to differentiate which makes learning easier.\nIt can be problematic when the input signal has very large positive or negative values. At those points the derivative is very close to 0 and learning becomes very slow.\nHyperbolic function This is another smooth function with a range of values between -1 and 1. This is interesting because sometimes a signal might have a reverse effect on the output and the hyperbolic function allows us to include this type of relationship in the network.\nRectified linear unit (ReLU) This function is very simple aned fast to compute. This allows us to work with larger and more complex models that, given enough data, can produce better results overall."
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-neural-network",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-a-neural-network",
    "title": "Deep Learning - Notes",
    "section": "What is a neural network?",
    "text": "What is a neural network?\n\nA neural network is a directed system of connected neurons which is capable of propagating a signal received from the environment and produces an output signal.\n\nThe neurons are generally organized in different layers which allow us to break free from linearity.\nLinearity means that we are also assuming monotonicity. In other words, an increase in an input value always increases the output value if the corresponding weight is positive (and will always decrease the output value if the weight is negative).\nThis works fine in examples like the one seen above where we turn on a lightbulb depending on how much energy we get from each battery.\nBut this makes less sense in other cases. For examples very pronounced sales spikes in an online marketplace followed by no sales at all could be indicative of fraud. A positive value for a feature describing this type of spikes should increase the probability of fraud.\nBut is this always the case?\nWhat if we additionally know that the owner of the store is a web celeb (ÁΩëÁ∫¢) on Alibaba? In that case the spikes above are expected, and therefore a positive value for this feature should decrease the probability of fraud while any other behavior might be suspicious.\nLet‚Äôs build a neural network step by step:\nThe simplest example is a single neuron with only one parameter which does not modify the signal in any way:\n\n\nCode\ndef nn(x, w):\n    return w*x\n\nprint(nn(3, 6))\n\n\n18\n\n\nMost processes we find in the real world are not this simple. We need to take into account multiple input features.\nI love music, imagine I want to predict if I will like a particular song or not. We can quickly multiply each input value by its corresponding weight using a dot product.\n\n\nCode\ndef nn(X, W):\n    return W.dot(X)\n\nx1 = 3.5 # length of songs in minutes\nx2 = 0 # binary value indicating if the genre is jazz or not\nx3 = 0 # binary value indicating if the artist is Nicki Minaj or not\nx4 = 1 # binary value indicating if the artist is Alan Walker\n\nX = np.array([x1, x2, x3, x4])\nW = np.array([1., 0.3, -3, 4])\n\nprint(nn(X, W))\n\n\n7.5\n\n\nWhat is interesting above is that the weights actually encode certain information about my taste in music. I really don‚Äôt like when a song is too short and, in general, I‚Äôm not fond of music by Nicki Minaj (notice the negative weight). But I do love music by Alan Walker. Jazz is nice but I don‚Äôt love it.\nNow the real question is, how do we know the values of these weights. In this case, I used my deep expertise regarding my own music taste to set the weights. But we would usually be more interested in doing this for the users of our streaming platform at scale. Millions of people we have never and will probably never meet in person.\nSo, first we‚Äôre going to change the output. We want to predict if a particular user is going to like a song or not. Using the sigmoid function we can directly compare the true values (0 or 1) to our predictions (floats between 0 and 1).\nWe will also initialize the weights randomly and print out an error value showing how close our prediction is to the true value:\n\n\nCode\n\ndef nn(X, y, W): \n    prediction = W.dot(X)\n    error = y-prediction\n    \n    print('Predicted value:', round(prediction, 3))\n    print('Error:', round(error, 3))\n\nx1 = 3.5 # length of songs in minutes\nx2 = 0 # binary value indicating if the genre is jazz or not\nx3 = 0 # binary value indicating if the artist is Nicki Minaj or not\nx4 = 1 # binary value indicating if the artist is Alan Walker\n\nX = np.array([x1, x2, x3, x4])\ny = 5\nW = np.random.random(size=4)*2-1\n\nnn(X, y, W)\n\n\nPredicted value: -1.468\nError: 6.468\n\n\nIt‚Äôs now time to start talking about learning or how our neural network is going to choose the best parameters during training to make accurate predictions.\nWe will follow a very simple framework of three steps:\n\nPredict\nCompare\nLearn\n\nThis time we will add multiple iterations to our algorithm and we will do the three steps above during each iteration.\n\n\nCode\ndef nn(X, y, W): \n    print(f'True value: {y}\\n')\n    \n    for i in range(10):\n        prediction = W.dot(X)\n        error = y-prediction\n\n        print('Predicted value:', round(prediction, 3))\n        print('Error:', round(error, 3))\n        \n        if error &gt; 0:\n            W += 0.01\n        else:\n            W -= 0.01  \n\nx1 = 3.5 # length of songs in minutes\nx2 = 0 # binary value indicating if the genre is jazz or not\nx3 = 0 # binary value indicating if the artist is Nicki Minaj or not\nx4 = 1 # binary value indicating if the artist is Alan Walker\n\nX = np.array([x1, x2, x3, x4])\ny = 1\nW = np.random.random(size=4)*2-1\n\nnn(X, y, W)\n\n\nTrue value: 1\n\nPredicted value: 0.116\nError: 0.884\nPredicted value: 0.161\nError: 0.839\nPredicted value: 0.206\nError: 0.794\nPredicted value: 0.251\nError: 0.749\nPredicted value: 0.296\nError: 0.704\nPredicted value: 0.341\nError: 0.659\nPredicted value: 0.386\nError: 0.614\nPredicted value: 0.431\nError: 0.569\nPredicted value: 0.476\nError: 0.524\nPredicted value: 0.521\nError: 0.479\n\n\nHere we are doing hot & cold learning. After each prediction we are making a comparison just like in the classical game so that we get ‚Äúhotter‚Äù or closer to the true answer after each guess.\nThe way we are calculating the error makes it negative if it was too high or positive if our guess was too low.\nWe can now be a bit smarter in the way we learn. Instead of guessing and trying to jiggle the output to both sides, we can use the gradient to guide us:\n\n\nCode\ndef nn(X, y, W): \n    print(f'True values: {y}\\n')\n    n = y.shape[1]\n    \n    for i in range(100):\n        predictions = W.dot(X.T)\n        mse = (1/2)*np.mean((y-predictions)**2)\n        \n        if i % 10 == 0:\n            print('Mean Squared Error:', round(mse, 3))\n        \n        dW = -(1/n)*(y-predictions).dot(X)\n        W -= 0.1*dW\n          \n    print('\\nW:', np.round(W, 3))\n    print('\\nFinal predictions:', np.round(predictions, 2))\n\nX = np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [1, 1]])\n\ny = np.array([[1, 0, 0, 1]])\nW = np.random.randn(1, 2)\n\nnn(X, y, W)\n\n\nTrue values: [[1 0 0 1]]\n\nMean Squared Error: 0.234\nMean Squared Error: 0.132\nMean Squared Error: 0.078\nMean Squared Error: 0.046\nMean Squared Error: 0.028\nMean Squared Error: 0.017\nMean Squared Error: 0.01\nMean Squared Error: 0.006\nMean Squared Error: 0.004\nMean Squared Error: 0.002\n\nW: [[0.927 0.073]]\n\nFinal predictions: [[0.92 0.07 0.   1.  ]]\n\n\nThis version led us quickly very close to the correct answers. But we now have another problem. The previous problem was very easy to learn because one of the features had a direct 1-on-1 relationship with an output. A simple linear function was capable of leveraging that feature.\nLet‚Äôs look at the next example:\n\n\nCode\nX = np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [1, 1]])\n\ny = np.array([[1, 1, 0, 0]])\nW = np.random.randn(1, 2)\n\nnn(X, y, W)\n\n\nTrue values: [[1 1 0 0]]\n\nMean Squared Error: 0.863\nMean Squared Error: 0.572\nMean Squared Error: 0.408\nMean Squared Error: 0.311\nMean Squared Error: 0.254\nMean Squared Error: 0.219\nMean Squared Error: 0.198\nMean Squared Error: 0.186\nMean Squared Error: 0.178\nMean Squared Error: 0.174\n\nW: [[0.462 0.204]]\n\nFinal predictions: [[0.47 0.2  0.   0.67]]\n\n\nMSE might not be extremely high but the answers are terrible. This is because the new problem we‚Äôre working with cannot be solved with a simple linear function. So, to solve this we can add a hidden layer with an activation function (more on why this works can be found further down):\n\n\nCode\nnp.random.seed(2)\n\ndef relu(x):\n    return (x&gt;0)*x\n\ndef relu_prime(x):\n    return x&gt;0\n\ndef nn(X, y, hidden_size , lr=0.01): \n    print(f'True values: {y}\\n')\n    n = y.shape[1]\n    \n    W1 = np.random.randn(X.shape[1], hidden_size)\n    W2 = np.random.randn(hidden_size, 1)\n    \n    for i in range(100):\n        z1 = X.dot(W1)\n        a1 = relu(z1)\n        \n        z2 = a1.dot(W2) # predictions\n        mse = (1/2)*np.mean((y-z2)**2)\n        \n        if i % 10 == 0:\n            print('Mean Squared Error:', round(mse, 3))\n        \n        dZ2 = (y-z2)\n        dW2 = a1.T.dot(dZ2)\n        W2 += lr*dW2\n        \n        dZ1 = (dZ2.dot(W2.T))*relu_prime(a1)\n        dW1 = X.T.dot(dZ1)\n        W1 += lr*dW1 #(1/n)\n    \n    print('\\nW1:', np.round(W1, 3))\n    print('W2:', np.round(W2, 3))\n    \n    print('\\nFinal predictions:', np.round(z2, 2))\n\nX = np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [1, 1]])\n\ny = np.array([[1, 1, 0, 0]]).T\n\nnn(X, y, 4)\n\n\nTrue values: [[1]\n [1]\n [0]\n [0]]\n\nMean Squared Error: 1.12\nMean Squared Error: 0.284\nMean Squared Error: 0.133\nMean Squared Error: 0.078\nMean Squared Error: 0.052\nMean Squared Error: 0.037\nMean Squared Error: 0.027\nMean Squared Error: 0.021\nMean Squared Error: 0.016\nMean Squared Error: 0.012\n\nW1: [[-0.417 -0.056 -2.136  0.599]\n [-1.793 -0.842  0.847 -1.302]]\nW2: [[-1.058]\n [-0.909]\n [ 0.876]\n [ 1.738]]\n\nFinal predictions: [[1.04]\n [0.74]\n [0.  ]\n [0.  ]]"
  },
  {
    "objectID": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-universality",
    "href": "posts/2021-03-01-dl-notes/2021-03-01-dl-notes.html#what-is-universality",
    "title": "Deep Learning - Notes",
    "section": "What is universality?",
    "text": "What is universality?\n\nUniversality is the ability to compute any arbitrary function.\n\nIt has been proven that any function we can think of can be approximated by a neural network with at least two layers.\nMichael Nielsen clarifies two caveats related to this idea of universality: 1. Universality doesn‚Äôt mean that the neural network is guaranteed to be exact, instead we are approximating the function and the more neurons we add the closer it gets to the true function. This means that for any function f(X) and any error threshold we define we can always find a neural network with output g(x) that satisfies:\n\\[ |g(x) - f(x)| &lt; \\epsilon \\]\n\nNeural networks approximate continuous functions. Functions that are not continuous with sharp and sudden jumps won‚Äôt be approximated by a neural network as a general rule. But that doesn‚Äôt mean that we can often use a continuous approximation that is good enough for our given purposes when trying to approximate a discontinuous function."
  }
]