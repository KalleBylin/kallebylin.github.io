{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Online Controlled Experiments\n",
    "> A Practical Guide to A/B Testing\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [books]\n",
    "- image: img/controlled experiments.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/controlled experiments.jpg\" alt=\"ab-testing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ The book in 3 sentences \n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Impressions\n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â˜˜ï¸ How the book changed me\n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœï¸ My top 3 quotes\n",
    "\n",
    "\n",
    "- \"Most who have run controlled experiments in customer-facing websites and applications have experienced this humbling reality: *we are poor at assessing the value of ideas*\"\n",
    "\n",
    "\n",
    "- Twyman's Law: \"Any figure that looks interesting or different is usually wrong\" - A.S.C. Ehrenberg || \"The more unusual or interesting the data, the more likely they are to have been the result of an error\" -  Catherine Marsh and Jane Elliott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ Summary + Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introductory topics for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction and motivation\n",
    "\n",
    "The book starts with an anecdote from Bing where an employee had proposed a new idea for how ad headlines should be displayed. The basic idea was to make the title longer by combining it with text from the first line under the title. Nobody seemed to think much of this idea and it was stashed into the backlog of experiments for six months before a software engineer decided to test the idea.\n",
    "\n",
    "This simple change ended up increasing revenue by 12%, which translated to more than $100 million dollars in the US alone annually without hurting other important metrics.\n",
    "\n",
    "A few takeaways from this event are that it's very hard to predict the value of an idea (this one ended up being forgotten during 6 months) and small changes can have huge impacts. For this reason it is important to have infrastructure in place that makes experimenting very cheap. Bing runs more than 10.000 experiments per year and the great majority don't have this kind of results.\n",
    "\n",
    "There are many types of **controlled experiments**. The main focus of this book is on a specific type of controlled experiment called an A/B test. The basic idea is to split users randomly between a control version and a variant.\n",
    "\n",
    "Another very important concept is the **Overall Evaluation Criterion (OEC)**. This could be a single metric or a combination of different metrics. The most important thing to remember is that the OEC has to be measurable in the short-term and believed to have a causal relationship with long-term success according to established strategic objectives.\n",
    "\n",
    "It is also **important to thoughtfully set up the randomization process**. This is the secret sauce of controlled experiments. The basic intuition behind this is that if there is no other factor influencing assignment apart from proper randomization, then on average we would expect both groups to be similar on all factors except the parameter we are changing. This means that any observed effect on the OEC should with very high probability be an effect of that change and nothing else.   \n",
    "\n",
    "This leads us into a conversation around **correlation, causality and trustworthiness**. The authors present a slightly silly but true example by sharing that the number of error messages has a strong inverse correlation with churn. Should we then increase the number of error messages or intentionally add bugs to the code of Office 365? Obviously not, because error messages don't cause lower churn. It turns out that power users tend to churn less and bump into more error messages just because they use the product more than other users. \n",
    "\n",
    "> \"We believe online controlled experiments are the best scientific way to establish causality with high probability\"\n",
    "\n",
    "\n",
    "The authors list a series of **ingredients that need to be present to be able to run a controlled experiment**:\n",
    "\n",
    "\n",
    "1. **Experimental units**, like users, that can be assigned to each variant without interference where users in one group could affect the actions of the users of the other group.\n",
    "\n",
    "\n",
    "2. **Enough experimental units**. The authors recommend to be at least in the thousands. The more units we have the more granular we can be in our experiments and discover smaller effects that are harder to detect.  \n",
    "\n",
    "\n",
    "3. **Key metrics** that are easy to understand, agreed upon and can be measured easily. If possible, the OEC should be used. A proxy metric can be used if the main metric is too difficult or slow to measure.\n",
    "\n",
    "\n",
    "4. **Easy changes**. The harder it is to create a variant, the harder it becomes to run a controlled experiment. This is one of the reasons software has become an ideal playground for running controlled experiments. \n",
    "\n",
    "\n",
    "The authors also present three tenets or key beliefs that any organization needs to run online controlled experiments:\n",
    "\n",
    "\n",
    "1. **The organization wants to make data-driven decisions and has formalized an OEC**\n",
    "\n",
    "Obviously, most people will say that data-driven decision-making is important for them. Still, in practice it is very common to plan, execute and declare success based on how much of the plan was accomplished while ignoring the impact on key metrics.\n",
    "\n",
    "In contrast, for data-driven decision-making to exist we need a clearly defined OEC (or set of OECs) that is measurable in the short term (1 to 2 weeks) and predictive of long-term success. This way we can quickly evaluate if what we are doing tactically and strategically is actually having a real impact on our OEC.\n",
    "\n",
    "\n",
    "2. **The organization is willing to invest in the infrastructure needed to run and test controlled experiments so that results are trustworthy**\n",
    "\n",
    "In some domains, like healthcare, it can be considered unethical or illegal to run certain controlled experiments. In other domains, like hardware production, changes can be slow and expensive.\n",
    "\n",
    "In general, software is a great option because it tends to be relatively easy to randomly split users between variants, log the results and iteratively add changes to the software. Still, the organization still has to be willing to invest in setting up the necessary tests and processes so that results are trustworthy at scale. \n",
    "\n",
    "\n",
    "3. **The organization recongizes that it is poor at assessing the value of ideas**\n",
    "\n",
    "Based on quotes from experts at Microsoft, Google, Netflix, Slack and others, the authors claim that most teams see experiment success rates of 10-33%, with most of them being on the lower end. In other words, we can expect 70-90% of our work to be thrown away due to poor or even negative results. \n",
    "\n",
    "> \"Most who have run controlled experiments in customer-facing websites and applications have experienced this humbling reality: *we are poor at assessing the value of ideas*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running and analyzing experiments\n",
    "\n",
    "This chapter focuses on the principles of designing, running and analyzing experiments. \n",
    "\n",
    "It starts with a story about a hypothetical e-commerce site where some employees have proposed the idea of adding coupon codes to the checkout process but another employee mentions that research has actually shown that coupon codes could have a negative effect (users get distracted or abandon the flow completely until they can get a code).\n",
    "\n",
    "The team decides to try out a **fake door or painted door apporach**. The analogy is to create a fake door or paint a door on the wall and see how many try to open it. This is a cheap way of testing an idea without having to build the whole feature first.\n",
    "\n",
    "Now the team has to choose their OEC. Revenue seems reasonable but the total sum can be misleading because, even if the process is randomized, one variant might end up with more users than the other which would skew the results. So revenue-per-user is a better choice. The authors go even deeper, clearly defining who should be counted as a user in this experiment.\n",
    "\n",
    "The final hypothesis to test would be \"Adding a coupon code option to the checkout process has a negative impact on revenue-per-user for users who start the purchase process\"\n",
    "\n",
    "At this point it is important to remember a few key concepts related to hypothesis testing:\n",
    "\n",
    "\n",
    "- When analyzing the key metric it is important to capture its **mean value and standard error**. In other words, we usually don't know the true value of the metric. So we use data estimate a mean value (other statistics can also be used) and the standard error tells us how variable this estimate is.\n",
    "\n",
    "\n",
    "- **Sensitivity** is the ability to detect statistically significant results (true positives). This concept goes hand in hand with **statistical power**, the probability of detecting a meaningful result when there really is one. This ability usually gets better with smaller standard errors. This makes intuitive sense. If we have a box filled with balls and almost all of them have the same color (low variablity) we can be pretty certain of the color of a ball if we pick it randomly. If we have a lot of different colors with equal distribution (high variablity) it becomes extremely difficult to guess the correct color. It is often possible to improve sensitivity by exposing more users to the experiment. We have to be careful though because are often additional costs with more users. \n",
    "\n",
    "\n",
    "- **Control vs treatment samples**. In this particular type of experiments we are usually not trying to estimate one value directly. We are often more interested in the difference between two values. In this case, we define a **Null hypothesis**, our assumption of how the world works, as the mean value of our control group being the same as the mean value of our treatment group. If we then look at the data and find this assumption to be unlikely (e.g. there is a very large difference between the sample means) we reject the null hypothesis and say that the difference is **statistically significant**.\n",
    "\n",
    "\n",
    "- To make the previous decision, we calculate the **p-value** for the difference. This is the probability of observing the difference between our sample means (or a more extreme difference) assuming that the null hypothesis is true. If this probability (the p-value) is very low, it means that observing this difference is very unlikely and that's why we go on to reject the null hypothesis. \n",
    "\n",
    "\n",
    "- For this to work, we have to decide on a threshold before we run our experiment, usually known as the **significance level**. A common standard is 0.05 which means that if there really is no difference between the two samples then we will correctly conclude this 95 out of each 100 experiments on average. Seen from another perspective, we will conclude that the difference is statistically significant even if there actually was no real difference (a false positive) about 5% of the time (note that this is different to stating that there is a 5% probability of a false positive if we reject the null hypothesis, which is a common misinterpretation).\n",
    "\n",
    "\n",
    "- Finally, there is also the question of **practical significance**. For a more personal example, would it be worth it to take on the challenge and costs of moving to another country with no family and friends for a 1% increase in salary keeping everything else equal (job requirements, benefits, etc.)? Probaby not, but a larger difference might actually be worth it. Here the question is, how large does the difference have to be so that it is practically meaningful?\n",
    "\n",
    "\n",
    "Now that we have a hypothesis, a significance boundary and a metric we also need to answer these questions to finish the **design of the experiment**:\n",
    "\n",
    "\n",
    "- What is the randomization unit? (Users is the most common)\n",
    "- What population of the randomization unit are we going to target? (All users vs a specific segment)\n",
    "- How large does the population of our experiment have to be? \n",
    "- How long do we have to run the experiment?\n",
    "\n",
    "There are multiple factors we can take into account for the size of the population. If the metric is binary (yes/no) or if we don't care about very granular differences we can use smaller populations. This is also the case for larger p-value thresholds or significance levels. In other words, if we are willing to make more mistakes then we don't need very large populations.\n",
    "\n",
    "Of course, this leads us to other kinds of questions. If the importance of the experiment is very high then we might need larger populations to increase our certainty of the results. The opposite is true if we suspect that the change could have a negative effect on users. In that case we might have to start with a smaller population and slowly increase the size to reduce risk of a negative impact.\n",
    "\n",
    "The duration of the experiment is also impacted by several factors. First of all, if we need more users we often have to allow the experiment to run for a longer period of time because not all users are exposed to the experiment simultaneously.\n",
    "\n",
    "A primacy effect might cause users to behave differently when they first see the change so we'll also have to wait longer to see if this behavior persists or not. It is also important to keep seasonality or day-of-week effects in mind. We might have to run the experiment longer than necessary from a theoretical stanpoint just to include different dates in the experiment.\n",
    "\n",
    "There are two main components for **running the experiment**:\n",
    "\n",
    "\n",
    "- **Infrastructure** to run the experiment. This includes the possibility of randomizing, presenting different variants, etc. \n",
    "\n",
    "- **Instrumentation** to log how users are interacting with the variants and to calculate how the OEC is affected by the experiment.\n",
    "\n",
    "\n",
    "Before analyzing the results, it is important to also run **sanity checks** on our experiment. \n",
    "\n",
    "This usually includes **guardrail metrics and invariants**. In the first case we might want to check the sample sizes so that they match with the expected proportions. Some guardrail metrics are also expected to be invariant for the experiments. For example, latency should not change between variants unless the change was specifically designed to alter the latency. \n",
    "\n",
    "With all this in place, we can now **interpret the results and make decisions**. The principles we have covered exist specifically so we can trust our experiments repeatedly and thus be able to make the right decisions based on the data.\n",
    "\n",
    "For this reason, it is important to understand the business implications of our experiments so that we can modify statistical and practical significance levels before we start the experiment. This includes the costs of building the feature if the experiment is successful, cost of maintenance, effects on other important metrics, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Twyman's law and experimentation trustworthiness\n",
    "\n",
    "\n",
    "> Twyman's Law: \"Any statistic that appears interesting is almost certainly a mistake\" - Paul Dickson\n",
    "\n",
    "From experience, **most extreme results tend to be due to computational errors, loss or duplication of data and instrumentation errors (e.g. logging)**. This chapter focuses on several common errors and how we can create tests to avoid falling into these mistakes. \n",
    "\n",
    "\n",
    "- **Lack of statistical power**. Here the mistake lies in assuming that there is no effect if the result of a hypothesis test is that a metric is not statistically significant. We simply don't have enough data to conclude this. For example, an effect can go unnoticed if we have a very small sample size. For that reason we have to understand what effect size is practically important to us and make sure that our test has enough power to detect that effect. \n",
    "\n",
    "\n",
    "- **Misinterpreting p-values**. Some common ways of misunderstanding this value is to state that if p-value=0.01 then there is a 1% chance that the null hypothesis is true or that there is a 1% of our result being a false positive after rejecting the null hypothesis. Compare these statements to the definitions above to prove that these conclusions don't follow from the given definition.\n",
    "\n",
    "\n",
    "- **Peeking at p-values**. This is a form of \"p-value hacking\" where it is common to peek at the p-values of an ongoing experiment (e.g. checking the p-value daily) until it is low enough to be significant and then reporting this value. Doing this consistently tends to bias the results.\n",
    "\n",
    "\n",
    "- **Multiple hypothesis tests**. This is a more general version of the previous concept and consists in comparing multiple versions of a test and picking the one with the lowest p-value to declare a significant result. other examples of this is to look at multiple metrics, segments, or even multiple iterations of the same experiment. \n",
    "\n",
    "\n",
    "- **Confidence intervals**. The authors present a few misunderstandings with respect to confidence intervals, one of which is assuming that a 95% confidence interval, for example, has a 95% probability of containing the true effect. The true value is either inside our outside the confidence interval, we already mentioned that the confidence interval and significance levels define how often we are willing to incorrectly reject the null hypothesis on average over multiple experiments.\n",
    "\n",
    "\n",
    "Other common issues are mentioned related to Internal validity (Violations of SUTVA, Survivorship bias, Intention-to-Treat, Sample Ratio Mismatch) and external validity (e.g. primacy and novelty effects) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
